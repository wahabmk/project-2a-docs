{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Mirantis k0rdent Documentation","text":""},{"location":"#introduction","title":"Introduction","text":"<p>k0rdent is focused on developing a consistent way to deploy and manage Kubernetes clusters at scale. One way to think of k0rdent is as a \"super control plane\" designed to manage other Kubernetes control planes. Another way to think of k0rdent is as a platform for Platform Engineering. If you are building an internal developer platform (IDP), need a way to manage Kubernetes clusters at scale in a centralized place, create Golden Paths, etc. k0rdent is a great way to do that.</p> <p>Whether you want to manage Kubernetes clusters on-premises, in the cloud, or a combination of both, k0rdent provides a consistent way to do so. With full life-cycle management, including provisioning, configuration, and maintenance, k0rdent is designed to be a repeatable and secure way to manage your Kubernetes clusters in a central location.</p>"},{"location":"#k0rdent-vs-project-2a-vs-hmc-naming","title":"k0rdent vs Project 2A vs HMC naming","text":"<p>k0rdent is the official name of an internal Mirantis project that was originally codenamed \"Project 2A\". During our initial skunkworks-style 3-month MVP push, the code was put into a repository named HMC, which stood for \"Hybrid Multi-Cluster Controller\". What is HMC became k0rdent Cluster Manager (kcm), but it may be a little confusing because the overall project was still called \"Project 2A\" or even \"HMC\" at times.</p> <p>So, to be clear, here are the names and components:</p> <ul> <li>k0rdent: the overall project name</li> <li>k0rdent Cluster Manager (kcm)</li> <li>k0rdent State Manager (ksm)<ul> <li>This is currently rolled into kcm, but will be split out in the   future</li> <li>ksm leverages Project Sveltos   for certain functionality</li> </ul> </li> <li>k0rdent Observability and FinOps (kof)</li> <li>Project 2A: the original codename of k0rdent (may occasionally show   up in some documentation)</li> <li>HMC or hmc: the original repository name for k0rdent and kcm   development (may occasionally show up in some documentation and code)</li> <li>motel: the original repository and codename for kof (may   occasionally show up in some documentation and code)</li> </ul>"},{"location":"#k0rdent-components","title":"k0rdent Components","text":"<p>The main components of k0rdent include:</p> <ul> <li> <p>k0rdent Cluster Manager (kcm)</p> <p>Deployment and life-cycle management of Kubernetes clusters, including configuration, updates, and other CRUD operations.</p> </li> <li> <p>k0rdent State Manager (ksm)</p> <p>Installation and life-cycle management of beach-head services, policy, Kubernetes API configurations and more.</p> </li> <li> <p>k0rdent Observability and FinOps (kof)</p> <p>Cluster and beach-head services monitoring, events and log management.</p> </li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>See the k0rdent Quick Start Guide.</p>"},{"location":"#supported-providers","title":"Supported Providers","text":"<p>k0rdent leverages the Cluster API provider ecosystem, the following providers have had <code>ProviderTemplates</code> created and validated, and more are in the works.</p> <ul> <li>AWS</li> <li>Azure</li> <li>vSphere</li> <li>OpenStack</li> </ul>"},{"location":"#development-documentation","title":"Development Documentation","text":"<p>Documentation related to development process and developer specific notes located in the main repository.</p>"},{"location":"glossary/","title":"Glossary","text":"<p>This glossary is a collection of terms related to k0rdent. It clarifies some of the unique  terms and concepts we use or explains more common ones that may need a little clarity in  the way we use them.</p>"},{"location":"glossary/#beach-head-services","title":"Beach-head services","text":"<p>We use the term to refer to those Kubernetes services that need to be installed on a  Kubernetes cluster to make it actually useful, for example: an ingress controller, CNI,  and/or CSI. While from the perspective of how they are deployed they are no different  from other Kubernetes services, we define them as distinct from the apps and services  deployed as part of the applications.</p>"},{"location":"glossary/#cluster-api-capi","title":"Cluster API (CAPI)","text":"<p>CAPI is a Kubernetes project that provides a declarative way to manage the lifecycle of  Kubernetes clusters. It abstracts the underlying infrastructure, allowing users to  create, scale, upgrade, and delete clusters using a consistent API. CAPI is extensible  via providers that offer infrastructure-specific functionality, such as AWS, Azure, and  vSphere.</p>"},{"location":"glossary/#capi-provider-see-also-infrastructure-provider","title":"CAPI provider (see also Infrastructure provider)","text":"<p>A CAPI provider is a Kubernetes CAPI extension that allows k0rdent to manage and drive  the creation of clusters on a specific infrastructure via API calls.</p>"},{"location":"glossary/#capa","title":"CAPA","text":"<p>CAPA stands for Cluster API Provider for AWS.</p>"},{"location":"glossary/#capg","title":"CAPG","text":"<p>CAPG stands for Cluster API Provider for Google Cloud.</p>"},{"location":"glossary/#capo","title":"CAPO","text":"<p>CAPO stands for Cluster API Provider for OpenStack.</p>"},{"location":"glossary/#capv","title":"CAPV","text":"<p>CAPV stands for Cluster API Provider for vSphere.</p>"},{"location":"glossary/#capz","title":"CAPZ","text":"<p>CAPZ stands for Cluster API Provider for Azure.</p>"},{"location":"glossary/#cloud-controller-manager","title":"Cloud Controller Manager","text":"<p>Cloud Controller Manager (CCM) is a Kubernetes component that embeds logic to manage a  specific infrastructure provider.</p>"},{"location":"glossary/#cluster-deployment","title":"Cluster Deployment","text":"<p>A Kubernetes cluster created and managed by k0rdent.</p>"},{"location":"glossary/#clusteridentity","title":"ClusterIdentity","text":"<p>ClusterIdentity is a Kubernetes object that references a Secret object containing  credentials for a specific infrastructure provider.</p>"},{"location":"glossary/#credential","title":"Credential","text":"<p>A <code>Credential</code> is a custom resource (CR) in kcm that supplies k0rdent with the necessary  credentials to manage a specific infrastructure. The credential object references other  CRs with infrastructure-specific credentials such as access keys, passwords,  certificates, etc. This means that a credential is specific to the CAPI provider that  uses it.</p>"},{"location":"glossary/#k0rdent-cluster-manager-kcm","title":"k0rdent Cluster Manager (kcm)","text":"<p>Deployment and life-cycle management of Kubernetes clusters, including configuration,  updates, and other CRUD operations.</p>"},{"location":"glossary/#k0rdent-observability-and-finops-kof","title":"k0rdent Observability and FinOps (kof)","text":"<p>Cluster and beach-head services monitoring, events and log management.</p>"},{"location":"glossary/#k0rdent-state-manager-ksm","title":"k0rdent State Manager (ksm)","text":"<p>Installation and life-cycle management of beach-head services, policy, Kubernetes API  configurations and more.</p>"},{"location":"glossary/#hosted-control-plane-hcp","title":"Hosted Control Plane (HCP)","text":"<p>An HCP is a Kubernetes control plane that runs outside of the clusters it manages.  Instead of running the control plane components (like the API server, controller  manager, and etcd) within the same cluster as the worker nodes, the control plane is  hosted on a separate, often centralized, infrastructure. This approach can provide  benefits such as easier management, improved security, and better resource utilization,  as the control plane can be scaled independently of the worker nodes.</p>"},{"location":"glossary/#infrastructure-provider-see-also-capi-provider","title":"Infrastructure provider (see also CAPI provider)","text":"<p>An infrastructure provider (aka <code>InfrastructureProvider</code>) is a Kubernetes custom  resource (CR) that defines the infrastructure-specific configuration needed for managing  Kubernetes clusters. It enables Cluster API (CAPI) to provision and manage clusters on  a specific infrastructure platform (e.g., AWS, Azure, VMware, OpenStack, etc.).</p>"},{"location":"glossary/#multi-cluster-service","title":"Multi-Cluster Service","text":"<p>The <code>MultiClusterService</code> is a custom resource used to manage deployment of beach-head  services across multiple clusters.</p>"},{"location":"glossary/#management-cluster","title":"Management Cluster","text":"<p>The Kubernetes cluster where k0rdent is installed and from which all other managed  clusters are managed.</p>"},{"location":"architecture/architecture/","title":"Architectural Overview","text":"<p>Below is a diagram that provides an overview of how k0rdent works.</p> <pre><code>---\ntitle: kcm Overview\n---\nerDiagram\n    USER ||--o{ kcm : uses\n    USER ||--o{ Template : assigns\n    Template ||--o{ kcm : \"used by\"\n    kcm ||--o{ CAPI : connects\n    CAPI ||--|{ CAPV : provider\n    CAPI ||--|{ CAPA : provider\n    CAPI ||--|{ CAPZ : provider\n    CAPI ||--|{ CAPO : provider\n    CAPI ||--|{ k0smotron : Bootstrap\n    K0smotron |o..o| CAPV : uses\n    K0smotron |o..o| CAPA : uses\n    K0smotron |o..o| CAPZ : uses\n    K0smotron |o..o| CAPO : uses</code></pre>"},{"location":"architecture/initialization/","title":"k0rdent initialization process","text":""},{"location":"architecture/initialization/#the-process","title":"The process","text":"<ol> <li>helm install kcm brings up the bootstrap components (yellow on the picture above)</li> <li>kcm-controller-manager set up webhooks to validate its CRs, then cert-manager handles the webhooks\u2019 certificates</li> <li>kcm-controller-manager generates Release object corresponding to the kcm helm chart version</li> <li>kcm-controller-manager (release-controller inside it) generates template objects (ProviderTemplate/ClusterTemplate/ServiceTemplate) corresponding to Release to be further processed</li> <li>kcm-controller-manager generates HelmRelease object for every template from p.3 (Important: it includes also kcm helm chart itself)</li> <li>Flux (source-controller and helm-controller pods) reconciles the HelmRelease objects i.e. installs all the helm charts referred to in the templates. After this point the deployment is completely controlled by Flux.</li> <li>kcm-controller-manager creates a Management object which refers to the above Release and the ProviderTemplate objects. Management object represents the k0rdent management cluster as a whole. The management cluster Day-2 operations (such as upgrade) are being executed by manipulating the Release and Management objects.</li> <li>kcm-controller-manager generates an empty AccessManagement object. AccessManagement defines access rules for ClusterTemplate/ServiceTemplate propagation across user namespaces. Further AccessManagement might be edited and used along with admin-created ClusterTemplateChain and ServiceTemplaitChain objects.</li> </ol>"},{"location":"clustertemplates/aws/hosted-control-plane/","title":"AWS Hosted control plane deployment","text":"<p>This section covers setting up for a k0smotron hosted control plane on AWS.</p>"},{"location":"clustertemplates/aws/hosted-control-plane/#prerequisites","title":"Prerequisites","text":"<ul> <li>Management Kubernetes cluster (v1.28+) deployed on AWS with kcm installed on it</li> <li>Default storage class configured on the management cluster</li> <li>VPC ID for the worker nodes</li> <li>Subnet ID which will be used along with AZ information</li> <li>AMI ID which will be used to deploy worker nodes</li> </ul> <p>Keep in mind that all control plane components for all cluster deployments will reside in the management cluster.</p>"},{"location":"clustertemplates/aws/hosted-control-plane/#networking","title":"Networking","text":"<p>The networking resources in AWS which are needed for a cluster deployment can be reused with a management cluster.</p> <p>If you deployed your AWS Kubernetes cluster using Cluster API Provider AWS (CAPA) you can obtain all the necessary data with the commands below or use the template found below in the kcm ClusterDeployment manifest generation section.</p> <p>If using the <code>aws-standalone-cp</code> template to deploy a hosted cluster it is recommended to use a <code>t3.large</code> or larger instance type as the <code>kcm-controller</code> and other provider controllers will need a large amount of resources to run.</p> <p>VPC ID</p> <pre><code>    kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{.spec.network.vpc.id}}'\n</code></pre> <p>Subnet ID</p> <pre><code>    kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{(index .spec.network.subnets 0).resourceID}}'\n</code></pre> <p>Availability zone</p> <pre><code>    kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{(index .spec.network.subnets 0).availabilityZone}}'\n</code></pre> <p>Security group <pre><code>    kubectl get awscluster &lt;cluster-name&gt; -o go-template='{{.status.networkStatus.securityGroups.node.id}}'\n</code></pre></p> <p>AMI id</p> <pre><code>    kubectl get awsmachinetemplate &lt;cluster-name&gt;-worker-mt -o go-template='{{.spec.template.spec.ami.id}}'\n</code></pre> <p>If you want to use different VPCs/regions for your management or managed clusters you should setup additional connectivity rules like VPC peering.</p>"},{"location":"clustertemplates/aws/hosted-control-plane/#kcm-clusterdeployment-manifest","title":"kcm ClusterDeployment manifest","text":"<p>With all the collected data your <code>ClusterDeployment</code> manifest will look similar to this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: aws-hosted-cp\nspec:\n  template: aws-hosted-cp-0-0-4\n  credential: aws-credential\n  config:\n    vpcID: vpc-0a000000000000000\n    region: us-west-1\n    publicIP: true\n    subnets:\n      - id: subnet-0aaaaaaaaaaaaaaaa\n        availabilityZone: us-west-1b\n        isPublic: true\n        natGatewayID: xxxxxx\n        routeTableId: xxxxxx\n      - id: subnet-1aaaaaaaaaaaaaaaa\n        availabilityZone: us-west-1b\n        isPublic: false\n        routeTableId: xxxxxx\n    instanceType: t3.medium\n    securityGroupIDs:\n      - sg-0e000000000000000\n</code></pre> <p>Note</p> <p> In this example we're using the <code>us-west-1</code> region, but you should use the region of your VPC.</p>"},{"location":"clustertemplates/aws/hosted-control-plane/#kcm-clusterdeployment-manifest-generation","title":"kcm ClusterDeployment manifest generation","text":"<p>Grab the following <code>ClusterDeployment</code> manifest template and save it to a file named <code>clusterdeployment.yaml.tpl</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: aws-hosted\nspec:\n  template: aws-hosted-cp-0-0-4\n  credential: aws-credential\n  config:\n    vpcID: \"{{.spec.network.vpc.id}}\"\n    region: \"{{.spec.region}}\"\n    subnets:\n    {{- range $subnet := .spec.network.subnets }}\n      - id: \"{{ $subnet.resourceID }}\"\n        availabilityZone: \"{{ $subnet.availabilityZone }}\"\n        isPublic: {{ $subnet.isPublic }}\n        {{- if $subnet.isPublic }}\n        natGatewayId: \"{{ $subnet.natGatewayId }}\"\n        {{- end }}\n        routeTableId: \"{{ $subnet.routeTableId }}\"\n        zoneType: \"{{ $subnet.zoneType }}\"\n    {{- end }}\n    instanceType: t3.medium\n    securityGroupIDs:\n      - \"{{.status.networkStatus.securityGroups.node.id}}\"\n</code></pre> <p>Then run the following command to create the <code>clusterdeployment.yaml</code>:</p> <pre><code>kubectl get awscluster cluster -o go-template=\"$(cat clusterdeployment.yaml.tpl)\" &gt; clusterdeployment.yaml\n</code></pre>"},{"location":"clustertemplates/aws/hosted-control-plane/#deployment-tips","title":"Deployment Tips","text":"<ul> <li>Ensure kcm templates and the controller image are somewhere public and   fetchable.</li> <li>For installing the kcm charts and templates from a custom repository, load   the <code>kubeconfig</code> from the cluster and run the commands:</li> </ul> <p><pre><code>KUBECONFIG=kubeconfig IMG=\"ghcr.io/k0rdent/kcm/controller-ci:v0.0.1-179-ga5bdf29\" REGISTRY_REPO=\"oci://ghcr.io/k0rdent/kcm/charts-ci\" make dev-apply\nKUBECONFIG=kubeconfig make dev-templates\n</code></pre> * The infrastructure will need to manually be marked <code>Ready</code> to get the   <code>MachineDeployment</code> to scale up.  You can patch the <code>AWSCluster</code> kind using   the command:</p> <pre><code>KUBECONFIG=kubeconfig kubectl patch AWSCluster &lt;hosted-cluster-name&gt; --type=merge --subresource status --patch 'status: {ready: true}' -n kcm-system\n</code></pre> <p>For additional information on why this is required click here.</p>"},{"location":"clustertemplates/aws/template-parameters/","title":"AWS template parameters","text":""},{"location":"clustertemplates/aws/template-parameters/#aws-ami","title":"AWS AMI","text":"<p>By default AMI ID will be looked up automatically using the latest Amazon Linux 2 image.</p> <p>You can override lookup parameters to search your desired image automatically or use AMI ID directly. If both AMI ID and lookup parameters are defined AMI ID will have higher precedence.</p>"},{"location":"clustertemplates/aws/template-parameters/#image-lookup","title":"Image lookup","text":"<p>To configure automatic AMI lookup 3 parameters are used:</p> <ul> <li><code>.imageLookup.format</code> - used directly as value for the <code>name</code> filter (see the describe-images filters).</li> <li> <p>Supports substitutions for <code>{{.BaseOS}}</code> and <code>{{.K8sVersion}}</code> with the base OS and kubernetes version, respectively.</p> </li> <li> <p><code>.imageLookup.org</code> - AWS org ID which will be used as value for the <code>owner-id</code> filter.</p> </li> <li> <p><code>.imageLookup.baseOS</code> - will be used as value for <code>{{.BaseOS}}</code> substitution in the <code>.imageLookup.format</code> string.</p> </li> </ul>"},{"location":"clustertemplates/aws/template-parameters/#ami-id","title":"AMI ID","text":"<p>AMI ID can be directly used in the <code>.amiID</code> parameter.</p>"},{"location":"clustertemplates/aws/template-parameters/#capa-prebuilt-amis","title":"CAPA prebuilt AMIs","text":"<p>Use <code>clusterawsadm</code> to get available AMIs to deploy cluster deployment:</p> <pre><code>clusterawsadm ami list\n</code></pre> <p>For details, see Pre-built Kubernetes AMIs.</p>"},{"location":"clustertemplates/aws/template-parameters/#ssh-access-to-cluster-nodes","title":"SSH access to cluster nodes","text":"<p>To access the nodes using the SSH protocol, several things should be configured:</p> <ul> <li>An SSH key added in the region where you want to deploy the cluster</li> <li>Bastion host is enabled</li> </ul>"},{"location":"clustertemplates/aws/template-parameters/#ssh-keys","title":"SSH keys","text":"<p>Only one SSH key is supported and it should be added in AWS prior to creating the <code>ClusterDeployment</code> object. The name of the key should then be placed under <code>.spec.config.sshKeyName</code>.</p> <p>The same SSH key will be used for all machines and a bastion host.</p> <p>To enable bastion you should add <code>.spec.config.bastion.enabled</code> option in the <code>ClusterDeployment</code> object to <code>true</code>.</p> <p>Full list of the bastion configuration options could be fould in CAPA docs.</p> <p>The resulting <code>ClusterDeployment</code> can look like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: aws-standalone-cp-0-0-5\n  credential: aws-cred\n  config:\n    sshKeyName: foobar\n    bastion:\n      enabled: true\n...\n</code></pre>"},{"location":"clustertemplates/aws/template-parameters/#eks-templates","title":"EKS templates","text":"<p>Warning</p> <p> When deploying EKS cluster please note that additional steps may be needed for proper VPC removal.</p> <p>EKS templates use the parameters similar to AWS and resulting EKS <code>ClusterDeployment</code> can look like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: aws-eks-0-0-3\n  credential: aws-cred\n  config:\n    sshKeyName: foobar\n    region: ${AWS_REGION}\n    workersNumber: 1\n...\n</code></pre>"},{"location":"clustertemplates/aws/vpc-removal/","title":"Non-removed VPC","text":"<p>A bug was fixed in CAPA (Cluster API Provider AWS) for VPC removal: kubernetes-sigs/cluster-api-provider-aws#5192</p> <p>It is possible to deal with non-deleted VPCs the following ways:</p>"},{"location":"clustertemplates/aws/vpc-removal/#applying-ownership-information-on-vpcs","title":"Applying ownership information on VPCs","text":"<p>When VPCs have owner information, all AWS resources will be removed when k0rdent EKS cluster is deleted. So, after provisioning EKS cluster the operator can go and set tags (i.e. <code>tag:Owner</code>) and it will be sufficient for CAPA to manage them.</p>"},{"location":"clustertemplates/aws/vpc-removal/#guardduty-vpce","title":"GuardDuty VPCE","text":"<p>Another way to prevent an issue with non-deleted VPCs is to disable GuardDuty. GuardDuty creates an extra VPCE (VPC Endpoint) not managed by CAPA and when CAPA starts EKS cluster removal, this VPCE is not removed.</p>"},{"location":"clustertemplates/aws/vpc-removal/#manual-removal-of-vpcs","title":"Manual removal of VPCs","text":"<p>When it is impossible to turn off GuardDuty or applying ownership tags is not permitted, it is needed to remove VPCs manually.</p> <p>The sign of \u201cstuck\u201d VPC looks like a hidden \u201cDelete\u201d button. </p> <p>Opening \u201cNetwork Interfaces\u201d and attempting to detach an interface shows disable \u201cDetach\u201d button: </p> <p>It is required to get to VPC endpoints screen and remove the end-point:  </p> <p></p> <p>Wait until VPCE is completely removed, all network interfaces disappear. </p> <p>Now VPC can be finally removed: </p>"},{"location":"clustertemplates/azure/hosted-control-plane/","title":"Azure Hosted control plane (k0smotron) deployment","text":""},{"location":"clustertemplates/azure/hosted-control-plane/#prerequisites","title":"Prerequisites","text":"<ul> <li>Management Kubernetes cluster (v1.28+) deployed on Azure with kcm installed     on it</li> <li>Default storage class configured on the management cluster</li> </ul> <p>Keep in mind that all control plane components for all managed clusters will reside in the management cluster.</p>"},{"location":"clustertemplates/azure/hosted-control-plane/#pre-existing-resources","title":"Pre-existing resources","text":"<p>Certain resources will not be created automatically in a hosted control plane scenario thus they should be created in advance and provided in the <code>ClusterDeployment</code> object. You can reuse these resources with management cluster as described below.</p> <p>If you deployed your Azure Kubernetes cluster using Cluster API Provider Azure (CAPZ) you can obtain all the necessary data with the commands below:</p> <p>Location</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.location}}'\n</code></pre> <p>Subscription ID</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.subscriptionID}}'\n</code></pre> <p>Resource group</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.resourceGroup}}'\n</code></pre> <p>vnet name</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{.spec.networkSpec.vnet.name}}'\n</code></pre> <p>Subnet name</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).name}}'\n</code></pre> <p>Route table name</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).routeTable.name}}'\n</code></pre> <p>Security group name</p> <pre><code>kubectl get azurecluster &lt;cluster-name&gt; -o go-template='{{(index .spec.networkSpec.subnets 1).securityGroup.name}}'\n</code></pre>"},{"location":"clustertemplates/azure/hosted-control-plane/#kcm-clusterdeployment-manifest","title":"kcm ClusterDeployment manifest","text":"<p>With all the collected data your <code>ClusterDeployment</code> manifest will look similar to this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp-0-0-2\n  credential: azure-credential\n  config:\n    location: \"westus\"\n    subscriptionID: ceb131c7-a917-439f-8e19-cd59fe247e03\n    vmSize: Standard_A4_v2\n    resourceGroup: mgmt-cluster\n    network:\n      vnetName: mgmt-cluster-vnet\n      nodeSubnetName: mgmt-cluster-node-subnet\n      routeTableName: mgmt-cluster-node-routetable\n      securityGroupName: mgmt-cluster-node-nsg\n</code></pre> <p>To simplify creation of the ClusterDeployment object you can use the template below:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: azure-hosted-cp\nspec:\n  template: azure-hosted-cp-0-0-2\n  credential: azure-credential\n  config:\n    location: \"{{.spec.location}}\"\n    subscriptionID: \"{{.spec.subscriptionID}}\"\n    vmSize: Standard_A4_v2\n    resourceGroup: \"{{.spec.resourceGroup}}\"\n    network:\n      vnetName: \"{{.spec.networkSpec.vnet.name}}\"\n      nodeSubnetName: \"{{(index .spec.networkSpec.subnets 1).name}}\"\n      routeTableName: \"{{(index .spec.networkSpec.subnets 1).routeTable.name}}\"\n      securityGroupName: \"{{(index .spec.networkSpec.subnets 1).securityGroup.name}}\"\n</code></pre> <p>Then you can render it using the command:</p> <pre><code>kubectl get azurecluster &lt;management-cluster-name&gt; -o go-template=\"$(cat template.yaml)\"\n</code></pre>"},{"location":"clustertemplates/azure/hosted-control-plane/#cluster-creation","title":"Cluster creation","text":"<p>After applying <code>ClusterDeployment</code> object you require to manually set the status of the <code>AzureCluster</code> object due to current limitations (see k0sproject/k0smotron#668).</p> <p>To do so you need to execute the following command:</p> <pre><code>kubectl patch azurecluster &lt;cluster-name&gt; --type=merge --subresource status --patch 'status: {ready: true}'\n</code></pre>"},{"location":"clustertemplates/azure/hosted-control-plane/#important-notes-on-the-cluster-deletion","title":"Important notes on the cluster deletion","text":"<p>Because of the aforementioned limitation you also need to make manual steps in order to properly delete cluster.</p> <p>Before removing the cluster make sure to place custom finalizer onto <code>AzureCluster</code> object. This is needed to prevent it from being deleted instantly which will cause cluster deletion to stuck indefinitely.</p> <p>To place finalizer you can execute the following command:</p> <pre><code>kubectl patch azurecluster &lt;cluster-name&gt; --type=merge --patch 'metadata: {finalizers: [manual]}'\n</code></pre> <p>When finalizer is placed you can remove the <code>ClusterDeployment</code> as usual. Check that all <code>AzureMachines</code> objects are deleted successfully and remove finalizer you've placed to finish cluster deletion.</p> <p>In case if have orphaned <code>AzureMachines</code> left you have to delete finalizers on them manually after making sure that no VMs are present in Azure.</p> <p>Note</p> <p> Since Azure admission prohibits orphaned objects mutation you'll have to disable it by deleting it's <code>mutatingwebhookconfiguration</code></p>"},{"location":"clustertemplates/azure/template-parameters/","title":"Azure machine parameters","text":""},{"location":"clustertemplates/azure/template-parameters/#ssh","title":"SSH","text":"<p>SSH public key can be passed to <code>.spec.config.sshPublicKey</code> (in case of hosted CP) parameter or <code>.spec.config.controlPlane.sshPublicKey</code> and <code>.spec.config.worker.sshPublicKey</code> parameters (in case of standalone CP) of the <code>ClusterDeployment</code> object.</p> <p>It should be encoded in base64 format.</p>"},{"location":"clustertemplates/azure/template-parameters/#vm-size","title":"VM size","text":"<p>Azure supports various VM sizes which can be retrieved with the following command:</p> <pre><code>az vm list-sizes --location \"&lt;location&gt;\" -o table\n</code></pre> <p>Then desired VM size could be passed to the:</p> <ul> <li><code>.spec.config.vmSize</code> - for hosted CP deployment.</li> <li><code>.spec.config.controlPlane.vmSize</code> - for control plane nodes in the standalone   deployment.</li> <li><code>.spec.config.worker.vmSize</code> - for worker nodes in the standalone deployment.</li> </ul> <p>Example: Standard_A4_v2</p>"},{"location":"clustertemplates/azure/template-parameters/#root-volume-size","title":"Root Volume size","text":"<p>Root volume size of the VM (in GB) can be changed through the following parameters:</p> <ul> <li><code>.spec.config.rootVolumeSize</code> - for hosted CP deployment.</li> <li><code>.spec.config.controlPlane.rootVolumeSize</code> - for control plane nodes in the   standalone deployment.</li> <li><code>.spec.config.worker.rootVolumeSize</code> - for worker nodes in the standalone   deployment.</li> </ul> <p>Default value: 30</p> <p>Please note that this value can't be less than size of the root volume which defined in your image.</p>"},{"location":"clustertemplates/azure/template-parameters/#vm-image","title":"VM Image","text":"<p>You can define the image which will be used for you machine using the following parameters:</p> <ul> <li><code>.spec.config.image</code> - for hosted CP deployment.</li> <li><code>.spec.config.controlPlane.image</code> - for control plane nodes in the standalone   deployment.</li> <li><code>.spec.config.worker.image</code> - for worker nodes in the standalone deployment.</li> </ul> <p>There are multiple self-excluding ways to define the image source (e.g. Azure Compute Gallery, Azure Marketplace, etc.).</p> <p>Detailed information regarding image can be found in CAPZ documentation</p> <p>By default, the latest official CAPZ Ubuntu based image is used.</p>"},{"location":"clustertemplates/openstack/template-parameters/","title":"OpenStack Machine parameters","text":""},{"location":"clustertemplates/openstack/template-parameters/#clusterdeployment-parameters","title":"ClusterDeployment Parameters","text":"<p>To deploy an OpenStack cluster, the following parameters are critical in the <code>ClusterDeployment</code> resource:</p> Parameter Example Description <code>.spec.credential</code> <code>openstack-cluster-identity-cred</code> Reference to the Credential object. <code>.spec.template</code> <code>openstack-standalone-cp-0-0-1</code> Reference to the ClusterTemplate. <code>.spec.config.authURL</code> <code>https://keystone.yourorg.net/</code> Keystone authentication endpoint for OpenStack. <code>.spec.config.controlPlaneNumber</code> <code>3</code> Number of control plane nodes. <code>.spec.config.workersNumber</code> <code>2</code> Number of worker nodes."},{"location":"clustertemplates/openstack/template-parameters/#ssh-configuration","title":"SSH Configuration","text":"<p><code>sshPublicKey</code> is a reference name for an existing SSH key configured in OpenStack.</p> <ul> <li>ClusterDeployment: Specify the SSH public key using <code>.spec.config.controlPlane.sshPublicKey</code> and <code>.spec.config.worker.sshPublicKey</code> parameters (in case of standalone CP).</li> </ul>"},{"location":"clustertemplates/openstack/template-parameters/#machine-configuration","title":"Machine Configuration","text":"<p>Configurations for control plane and worker nodes are specified separately under <code>.spec.config.controlPlane</code> and <code>.spec.config.worker</code>:</p> Parameter Example Description <code>flavor</code> <code>m1.medium</code> OpenStack flavor for the instance. <code>image.filter.name</code> <code>ubuntu-22.04-x86_64</code> Name of the image. <code>sshPublicKey</code> <code>ramesses-pk</code> Reference name for an existing SSH key. <code>securityGroups.filter.name</code> <code>default</code> Security group for the instance. <p>[!NOTE] Ensure <code>.spec.credential</code> references the <code>Credential</code> object (see OpenStack-QuickStart) The recommended minimum vCPU value for the control plane flavor is 2, while for the worker node flavor, it is 1. For detailed information, refer to the machine-flavor CAPI docs.</p>"},{"location":"clustertemplates/openstack/template-parameters/#example-clusterdeployment","title":"Example ClusterDeployment","text":"<pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-openstack-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: openstack-standalone-cp-0-0-2\n  credential: openstack-cluster-identity-cred\n  config:\n    controlPlaneNumber: 1\n    workersNumber: 1\n    controlPlane:\n      sshPublicKey: my-public-key\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    worker:\n      sshPublicKey: my-public-key\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    authURL: https://my-keystone-openstack-url.com\n    identityRef:\n      name: openstack-cloud-config\n      cloudName: openstack\n      region: RegionOne\n</code></pre>"},{"location":"clustertemplates/vsphere/hosted-control-plane/","title":"Hosted control plane (k0smotron) deployment","text":""},{"location":"clustertemplates/vsphere/hosted-control-plane/#prerequisites","title":"Prerequisites","text":"<ul> <li>Management Kubernetes cluster (v1.28+) deployed on vSphere with kcm installed   on it</li> </ul> <p>Keep in mind that all control plane components for all managed clusters will reside in the management cluster.</p>"},{"location":"clustertemplates/vsphere/hosted-control-plane/#clusterdeployment-manifest","title":"ClusterDeployment manifest","text":"<p>Hosted CP template has mostly identical parameters with standalone CP, you can check them in the template parameters section.</p> <p>Important Note on Control Plane Endpoint IP Address</p> <p> The vSphere provider requires the control plane endpoint IP to be specified before deploying the cluster. Ensure that this IP matches the IP assigned to the k0smotron load balancer (LB) service. Provide the control plane endpoint IP to the k0smotron service via an annotation accepted by your LB provider (e.g., the <code>kube-vip</code> annotation in the example below).</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: vsphere-hosted-cp-0-0-2\n  credential: vsphere-credential\n  config:\n    vsphere:\n      server: vcenter.example.com\n      thumbprint: \"00:00:00\"\n      datacenter: \"DC\"\n      datastore: \"/DC/datastore/DC\"\n      resourcePool: \"/DC/host/vCluster/Resources/ResPool\"\n      folder: \"/DC/vm/example\"\n    controlPlaneEndpointIP: \"172.16.0.10\"\n    ssh:\n      user: ubuntu\n      publicKey: |\n        ssh-rsa AAA...\n    rootVolumeSize: 50\n    cpus: 2\n    memory: 4096\n    vmTemplate: \"/DC/vm/template\"\n    network: \"/DC/network/Net\"\n    k0smotron:\n      service:\n        annotations:\n          kube-vip.io/loadbalancerIPs: \"172.16.0.10\"\n</code></pre>"},{"location":"clustertemplates/vsphere/template-parameters/","title":"vSphere cluster template parameters","text":""},{"location":"clustertemplates/vsphere/template-parameters/#clusterdeployment-parameters","title":"ClusterDeployment parameters","text":"<p>To create a cluster deployment a number of parameters should be passed to the <code>ClusterDeployment</code> object.</p>"},{"location":"clustertemplates/vsphere/template-parameters/#parameter-list","title":"Parameter list","text":"<p>The following is the list of vSphere specific parameters, which are required for successful cluster creation.</p> Parameter Example Description <code>.spec.config.vsphere.server</code> <code>vcenter.example.com</code> Address of the vSphere instance <code>.spec.config.vsphere.thumbprint</code> <code>\"00:00:00:...\"</code> Certificate thumbprint <code>.spec.config.vsphere.datacenter</code> <code>DC</code> Datacenter name <code>.spec.config.vsphere.datastore</code> <code>/DC/datastore/DS</code> Datastore path <code>.spec.config.vsphere.resourcePool</code> <code>/DC/host/vCluster/Resources/ResPool</code> Resource pool path <code>.spec.config.vsphere.folder</code> <code>/DC/vm/example</code> Folder path <code>.spec.config.controlPlane.network</code> <code>/DC/network/vm_net</code> Network path for <code>controlPlane</code> <code>.spec.config.worker.network</code> <code>/DC/network/vm_net</code> Network path for <code>worker</code> <code>.spec.config.*.ssh.publicKey</code> <code>\"ssh-ed25519 AAAA...\"</code> SSH public key in <code>authorized_keys</code> format <code>.spec.config.*.vmTemplate</code> <code>/DC/vm/templates/ubuntu</code> VM template image path <code>.spec.config.controlPlaneEndpointIP</code> <code>172.16.0.10</code> <code>kube-vip</code> vIP which will be created for control plane endpoint <p>To obtain vSphere certificate thumbprint you can use the following command:</p> <pre><code>curl -sw %{certs} https://vcenter.example.com | openssl x509 -sha256 -fingerprint -noout | awk -F '=' '{print $2}'\n</code></pre> <p><code>govc</code>, a vSphere CLI, can also help to discover proper values for some of the parameters:</p> <pre><code># vsphere.datacenter\ngovc ls\n\n# vsphere.datastore\ngovc ls /*/datastore/*\n\n# vsphere.resourcePool\ngovc ls /*/host/*/Resources/*\n\n# vsphere.folder\ngovc ls -l /*/vm/**\n\n# controlPlane.network, worker.network\ngovc ls /*/network/*\n\n# *.vmTemplate\ngovc vm.info -t '*'\n</code></pre> <p>Note</p> <p> Follow official <code>govc</code> installation instructions from here, and <code>govc</code> usage guide is here.</p> <p>Minimal <code>govc</code> configuration requires setting: <code>GOVC_URL</code>, <code>GOVC_USERNAME</code>, <code>GOVC_PASSWORD</code> environment variables.</p>"},{"location":"clustertemplates/vsphere/template-parameters/#example-of-clusterdeployment-cr","title":"Example of ClusterDeployment CR","text":"<p>With all above parameters provided your <code>ClusterDeployment</code> can look like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: cluster-1\nspec:\n  template: vsphere-standalone-cp-0-0-2\n  credential: vsphere-credential\n  config:\n    vsphere:\n      server: vcenter.example.com\n      thumbprint: \"00:00:00\"\n      datacenter: \"DC\"\n      datastore: \"/DC/datastore/DC\"\n      resourcePool: \"/DC/host/vCluster/Resources/ResPool\"\n      folder: \"/DC/vm/example\"\n    controlPlaneEndpointIP: \"172.16.0.10\"\n\n    controlPlane:\n      ssh:\n        user: ubuntu\n        publicKey: |\n          ssh-rsa AAA...\n      rootVolumeSize: 50\n      cpus: 2\n      memory: 4096\n      vmTemplate: \"/DC/vm/template\"\n      network: \"/DC/network/Net\"\n\n    worker:\n      ssh:\n        user: ubuntu\n        publicKey: |\n          ssh-rsa AAA...\n      rootVolumeSize: 50\n      cpus: 2\n      memory: 4096\n      vmTemplate: \"/DC/vm/template\"\n      network: \"/DC/network/Net\"\n</code></pre>"},{"location":"clustertemplates/vsphere/template-parameters/#ssh","title":"SSH","text":"<p>Currently SSH configuration on vSphere expects that user is already created during template creation. Because of that you must pass username along with SSH public key to configure SSH access.</p> <p>SSH public key can be passed to <code>.spec.config.ssh.publicKey</code> (in case of hosted CP) parameter or <code>.spec.config.controlPlane.ssh.publicKey</code> and <code>.spec.config.worker.ssh.publicKey</code> parameters (in case of standalone CP) of the <code>ClusterDeployment</code> object.</p> <p>SSH public key must be passed literally as a string.</p> <p>Username can be passed to <code>.spec.config.controlPlane.ssh.user</code>, <code>.spec.config.worker.ssh.user</code> or <code>.spec.config.ssh.user</code> depending on you deployment model.</p>"},{"location":"clustertemplates/vsphere/template-parameters/#vm-resources","title":"VM resources","text":"<p>The following parameters are used to define VM resources:</p> Parameter Example Description <code>.rootVolumeSize</code> <code>50</code> Root volume size in GB (can't be less than one defined in the image) <code>.cpus</code> <code>2</code> Number of CPUs <code>.memory</code> <code>4096</code> Memory size in MB <p>The resource parameters are the same for hosted and standalone CP deployments, but they are positioned differently in the spec, which means that they're going to:</p> <ul> <li><code>.spec.config</code> in case of hosted CP deployment.</li> <li><code>.spec.config.controlPlane</code> in in case of standalone CP for control plane   nodes.</li> <li><code>.spec.config.worker</code> in in case of standalone CP for worker nodes.</li> </ul>"},{"location":"clustertemplates/vsphere/template-parameters/#vm-image-and-network","title":"VM Image and network","text":"<p>To provide image template path and network path the following parameters must be used:</p> Parameter Example Description <code>.vmTemplate</code> <code>/DC/vm/template</code> Image template path <code>.network</code> <code>/DC/network/Net</code> Network path <p>As with resource parameters the position of these parameters in the <code>ClusterDeployment</code> depends on deployment type and these parameters are used in:</p> <ul> <li><code>.spec.config</code> in case of hosted CP deployment.</li> <li><code>.spec.config.controlPlane</code> in in case of standalone CP for control plane   nodes.</li> <li><code>.spec.config.worker</code> in in case of standalone CP for worker nodes.</li> </ul>"},{"location":"credential/distribution/","title":"Credential Distribution System","text":"<p>The kcm system provides a mechanism to distribute <code>Credential</code> objects across namespaces using the <code>AccessManagement</code> object. This object defines a set of <code>accessRules</code> that determine how credentials are distributed.</p> <p>Each access rule specifies:</p> <ol> <li>The target namespaces where credentials should be delivered.</li> <li>A list of <code>Credential</code> names to distribute to those namespaces.</li> </ol> <p>The kcm controller will copy the specified <code>Credential</code> objects from the system namespace to the target namespaces based on the <code>accessRules</code> in the <code>AccessManagement</code> spec.</p> <p>Info</p> <p> Access rules can also include <code>Cluster</code> and <code>Service</code> TemplateChains (<code>clusterTemplateChains</code> and <code>serviceTemplateChains</code>) to distribute templates to target namespaces. For more details, read: Template Life Cycle Management.</p>"},{"location":"credential/distribution/#how-to-configure-credential-distribution","title":"How to Configure Credential Distribution","text":"<p>To configure the distribution of <code>Credential</code> objects:</p> <ol> <li>Edit the <code>AccessManagement</code> object.</li> <li>Populate the <code>.spec.accessRules</code> field with the list of <code>Credential</code> names and the target namespaces.</li> </ol> <p>Here\u2019s an example configuration:</p> <pre><code>spec:\n  accessRules:\n  - targetNamespaces:\n      list:\n        - dev\n        - test\n    credentials:\n      - aws-demo\n      - azure-demo\n</code></pre> <p>In this example, the <code>aws-demo</code> and <code>azure-demo</code> <code>Credential</code> objects will be distributed to the <code>dev</code> and <code>test</code> namespaces.</p>"},{"location":"credential/main/","title":"Credential System","text":"<p>In order for infrastructure provider to work properly a correct credentials should be passed to it. The following describes how it is implemented in k0rdent.</p>"},{"location":"credential/main/#the-process","title":"The process","text":"<p>The following is the process of passing credentials to the system:</p> <ol> <li>Provider specific <code>ClusterIdentity</code> and <code>Secret</code> are created</li> <li><code>Credential</code> object is created referencing <code>ClusterIdentity</code> from step 1.</li> <li>The <code>Credential</code> object is then referenced in the <code>ClusterDeployment</code>.</li> <li>Optionally, certain credentials MAY be propagated to the <code>ClusterDeployment</code> after it is created.</li> </ol> <p>The following diagram illustrates the process:</p> <pre><code>flowchart TD\n  Step1[\"&lt;b&gt;Step 1&lt;/b&gt; (Lead Engineer):&lt;br/&gt;Create ClusterIdentity and Secret objects where ClusterIdentity references Secret\"]\n  Step1 --&gt; Step2[\"&lt;b&gt;Step 2&lt;/b&gt; (Any Engineer):&lt;br/&gt;Create Credential object referencing ClusterIdentity\"]\n  Step2 --&gt; Step3[\"&lt;b&gt;Step 3&lt;/b&gt; (Any Engineer):&lt;br/&gt;Create ClusterDeployment referencing Credential object\"]\n  Step3 --&gt; Step4[\"&lt;b&gt;Step 4&lt;/b&gt; (Any Engineer):&lt;br/&gt;Apply ClusterDeployment, wait for provisioning &amp; reconciliation, then propagate credentials to nodes if necessary\"]</code></pre> <p>By design steps 1 and 2 should be executed by the lead engineer who has access to the credentials. Thus credentials could be used by engineers without a need to have access to actual credentials or underlying resources, like <code>ClusterIdentity</code>.</p>"},{"location":"credential/main/#credential-object","title":"Credential object","text":"<p>The <code>Credential</code> object acts like a reference to the underlying credentials. It is namespace-scoped, which means that it must be in the same <code>Namespace</code> with the <code>ClusterDeployment</code> it is referenced in. Actual credentials can be located in any namespace.</p>"},{"location":"credential/main/#example","title":"Example","text":"<pre><code>---\napiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: azure-credential\n  namespace: dev\nspec:\n  description: \"Main Azure credentials\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: AzureClusterIdentity\n    name: azure-cluster-identity\n    namespace: kcm-system\n</code></pre> <p>In the example above <code>Credential</code> object is referencing <code>AzureClusterIdentity</code> which was created in the <code>kcm-system</code> namespace.</p> <p>The <code>.spec.description</code> field can be used to provide arbitrary description of the object, so user could make a decision which credentials to use if several are present.</p>"},{"location":"credential/main/#cloud-provider-credentials-propagation","title":"Cloud provider credentials propagation","text":"<p>Some components in the cluster deployment require cloud provider credentials to be passed for proper functioning. As an example Cloud Controller Manager (CCM) requires provider credentials to create load balancers and provide other functionality.</p> <p>This poses a challenge of credentials delivery. Currently <code>cloud-init</code> is used to pass all necessary credentials. This approach has several problems:</p> <ul> <li>Credentials stored unencrypted in the instance metadata.</li> <li>Rotation of the credentials is impossible without complete instance   redeployment.</li> <li>Possible leaks, since credentials are copied to several <code>Secret</code> objects   related to bootstrap data.</li> </ul> <p>To solve these problems in k0rdent we're using Sveltos controller which can render CCM template with all necessary data from CAPI provider resources (like <code>ClusterIdentity</code>) and can create secrets directly on the cluster deployment.</p> <p>Note</p> <p> CCM template examples can be found in <code>*-credentials.yaml</code> here. Look for ConfigMap object that has <code>projectsveltos.io/template: \"true\"</code> annotation and <code>*-resource-template</code> object name.</p> <p>This eliminates the need to pass anything credentials-related to <code>cloud-init</code> and makes it possible to rotate credentials automatically without the need for instance redeployment.</p> <p>Also this automation makes it possible to separate roles and responsibilities where only the lead engineer has access to credentials and other engineers can use them without seeing values and even any access to underlying infrastructure platform.</p> <p>The process is fully automated and credentials will be propagated automatically within the <code>ClusterDeployment</code> reconciliation process, user only needs to provide the correct Credential object.</p>"},{"location":"credential/main/#provider-specific-notes","title":"Provider specific notes","text":"<p>Since this feature depends on the provider some notes and clarifications are needed for each provider.</p> <p>Note</p> <p>More detailed research notes can be found here.</p>"},{"location":"credential/main/#aws","title":"AWS","text":"<p>Since AWS uses roles, which are assigned to instances, no additional credentials will be created.</p> <p>AWS provider supports 3 types of <code>ClusterIdentity</code>, which one to use depends on your specific use case. More information regarding CAPA <code>ClusterIdentity</code> resources could be found in CRD Reference.</p>"},{"location":"credential/main/#azure","title":"Azure","text":"<p>Currently Cluster API provider Azure (CAPZ) creates <code>azure.json</code> Secrets in the same namespace with <code>Cluster</code> object. By design they should be referenced in the <code>cloud-init</code> YAML later during bootstrap process.</p> <p>In k0rdent these Secrets aren't used and will not be added to the <code>cloud-init</code>, but engineers can access them unrestricted.</p>"},{"location":"credential/main/#openstack","title":"OpenStack","text":"<p>For OpenStack, CAPO relies on a clouds.yaml file. In k0rdent, you provide this file in a Kubernetes Secret that references OpenStack credentials (ideally application credentials for enhanced security). During reconciliation, kcm automatically generates the cloud-config required by OpenStack\u2019s cloud-controller-manager.</p> <p>For more details, refer to the kcm OpenStack Credential Propagation doc.</p>"},{"location":"credential/main/#adopted-cluster","title":"Adopted cluster","text":"<p>Credentials for adopted clusters consist of a secret containing a kubeconfig file to access the existing kubernetes cluster.  The kubeconfig file for the cluster should be contained in the value key of the secret object. The following is an example of  a secret which contains the kubeconfig for an adopted cluster. To create this secret, first create or obtain a kubeconfig file  for the cluster that is being adopted and then run the following command to base64 encode it:</p> <pre><code>cat kubeconfig | base64 -w 0\n</code></pre> <p>Once you have obtained a base64 encoded kubeconfig file create a secret:</p> <pre><code>apiVersion: v1\ndata:\n  value: &lt;base64 encoded kubeconfig file&gt;\nkind: Secret\nmetadata:\n  name: adopted-cluster-kubeconf\n  namespace: &lt;namespace&gt;\ntype: Opaque\n</code></pre>"},{"location":"disaster-recovery/customization/","title":"Customization","text":"<p>This section covers different topics of customization regarding the disaster recovery feature.</p>"},{"location":"disaster-recovery/customization/#velero-installation","title":"Velero installation","text":"<p>The <code>kcm</code> helm chart supplied with the <code>velero</code> helm chart and is enabled by default. There are 2 ways of customizing the chart values similar to the installation guide:</p> <ol> <li> <p>If installing using <code>helm</code> add corresponding parameters to the <code>helm install</code> command,    for example:</p> <pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm \\\n --version &lt;version&gt; \\\n --create-namespace \\\n --namespace kcm-system \\\n --set velero.initContainers[0].name=velero-plugin-for-&lt;PROVIDER NAME&gt; \\\n --set velero.initContainers[0].image=velero/velero-plugin-for-&lt;PROVIDER NAME&gt;:&lt;PROVIDER PLUGIN TAG&gt; \\\n --set velero.initContainers[0].volumeMounts[0].mountPath=/target \\\n --set velero.initContainers[0].volumeMounts[0].name=plugins\n</code></pre> </li> <li> <p>Create or modify the existing <code>Management</code> object in the <code>.spec.config.kcm</code>, for example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  core:\n    kcm:\n      config:\n        velero:\n         initContainers:\n           - name: velero-plugin-for-&lt;PROVIDER NAME&gt;\n             image: velero/velero-plugin-for-&lt;PROVIDER NAME&gt;:&lt;PROVIDER PLUGIN TAG&gt;\n             imagePullPolicy: IfNotPresent\n             volumeMounts:\n               - mountPath: /target\n                 name: plugins\n      # ...\n</code></pre> </li> </ol> <p>To fully disable <code>velero</code>, set the <code>velero.enabled</code> parameter to <code>false</code>.</p>"},{"location":"disaster-recovery/customization/#schedule-expression-format","title":"Schedule expression format","text":"<p>The <code>ManagementBackup</code> <code>.spec.schedule</code> field accepts a correct Cron expression, along with the nonstandard predefined scheduling definitions and an extra definition <code>@every</code> with a number and a valid time unit (valid time units are <code>ns, us (or \u00b5s), ms, s, m, h</code>).</p> <p>The following list contains <code>.spec.schedule</code> acceptable example values:</p> <ul> <li><code>0 */1 * * *</code> (standard Cron expression)</li> <li><code>@hourly</code> (nonstandard predefined definition)</li> <li><code>@every 1h</code> (extra definition)</li> </ul>"},{"location":"disaster-recovery/customization/#putting-extra-objects-in-a-backup","title":"Putting extra objects in a Backup","text":"<p>If a situation arises in which it is necessary to back up some additional objects in addition to those backed up by default, it is enough to add the following label <code>k0rdent.mirantis.com/component=\"kcm\"</code> to these objects.</p> <p>All objects containing the label will be automatically added to the backup.</p>"},{"location":"disaster-recovery/overview/","title":"Disaster Recovery Feature Overview","text":"<p>This document provides an overview of the disaster recovery feature used to back up and restore a <code>kcm</code> management cluster.</p> <p>The feature leverages <code>velero</code> for backup management on the backend and integrates with the <code>kcm</code> to ensure data persistence and recovery.</p>"},{"location":"disaster-recovery/overview/#motivation","title":"Motivation","text":"<p>The primary goal of this feature is to provide a reliable and efficient way to back up and restore <code>kcm</code> deployment in the event of a disaster that impacts the management cluster. By utilizing <code>velero</code> as the backup provider, we can ensure consistent backups across different cloud storage while maintaining the integrity of critical resources.</p> <p>The main goal of the feature is to provide:</p> <ul> <li>Backup: the ability to backup all configuration objects created and managed by the <code>kcm</code>   into an offsite location.</li> <li>Restore: the ability to create configuration objects from a specific Backup.</li> <li>Disaster Recovery: the ability to restore the <code>kcm</code> system on another management cluster   using restore capability, plus ensuring that clusters are not recreated or lost.</li> <li>Rollback: the possibility to manually restore after a specific event, e.g. a failed upgrade   of the <code>kcm</code>.</li> </ul>"},{"location":"disaster-recovery/overview/#velero-as-provider-for-backups","title":"Velero as Provider for Backups","text":"<p><code>Velero</code> is an open-source tool that simplifies backing up and restoring clusters as well as individual resources. It seamlessly integrates into the <code>kcm</code> management environment to provide robust disaster recovery capabilities.</p> <p>The <code>velero</code> instance is installed as part of the <code>kcm</code> and included in its helm chart, hence the installation process might be fully customized.</p> <p>The <code>kcm</code> manages the schedule and is responsible for collecting the sufficient data to be included in a backup.</p> <p>Customization options for the installation and backups are covered by the according section.</p>"},{"location":"disaster-recovery/overview/#scheduled-backups","title":"Scheduled Backups","text":""},{"location":"disaster-recovery/overview/#preparation","title":"Preparation","text":"<p>Before the creation of scheduled backups, several actions should be performed beforehand:</p> <ol> <li>Prepare a cloud storage to save backups to (e.g. <code>Amazon S3</code>).</li> <li>Create a <code>BackupStorageLocation</code>    object referencing a <code>Secret</code> with credentials to access the cloud storage    (if the multiple credentials feature is supported by the plugin).</li> </ol> <p>Example</p> <p>An example of a <code>BackupStorageLocation</code> and the related <code>Secret</code> for the <code>Amazon S3</code> and the <code>AWS</code> provider:</p> <pre><code> ---\n # Secret with the cloud storage credentials\n apiVersion: v1\n data:\n   # base64-encoded credentials, for the Amazon S3 in the following format:\n   # [default]\n   # aws_access_key_id = &lt;AWS_ACCESS_KEY&gt;\n   # aws_secret_access_key = &lt;AWS_SECRET_ACCESS_KEY&gt;\n   cloud: W2RlZmF1bHRdCmF3c19hY2Nlc3Nfa2V5X2lkID0gPEFXU19BQ0NFU1NfS0VZPgphd3Nfc2VjcmV0X2FjY2Vzc19rZXkgPSA8QVdTX1NFQ1JFVF9BQ0NFU1NfS0VZPgo=\n kind: Secret\n metadata:\n   name: cloud-credentials\n   namespace: kcm-system\n type: Opaque\n ---\n # Velero storage location\n apiVersion: velero.io/v1\n kind: BackupStorageLocation\n metadata:\n   name: aws-s3\n   namespace: kcm-system\n spec:\n   config:\n     region: &lt;your-region-name&gt;\n   default: true # optional, if not set, then storage location name must always be set in ManagementBackup\n   objectStorage:\n     bucket: &lt;your-bucket-name&gt;\n   provider: aws\n   backupSyncPeriod: 1m\n   credential:\n     name: cloud-credentials\n     key: cloud\n</code></pre> <p>Hint</p> <p> For more comprehensive examples and to familiarize yourself with limitations and caveats please follow the link to the official location documentation.</p>"},{"location":"disaster-recovery/overview/#create-backup","title":"Create Backup","text":"<p>To set a <code>ManagementBackup</code> object to create periodic backups, set the <code>.spec.schedule</code> field with a Cron expression. If the <code>.spec.schedule</code> is not set, the backup on demand will be created instead.</p> <p>Optionally, set the name of the <code>BackupStorageLocation</code> <code>.spec.backup.storageLocation</code>. The default location is the <code>BackupStorageLocation</code> object with <code>.spec.default</code> set to <code>true</code>.</p> <p>Example</p> <p>An example of the <code>ManagementBackup</code> object with a schedule and the storage location:</p> <pre><code> apiVersion: k0rdent.mirantis.com/v1alpha1\n kind: ManagementBackup\n metadata:\n   name: kcm\n spec:\n   schedule: \"0 */6 * * *\"\n   storageLocation: aws-s3\n</code></pre>"},{"location":"disaster-recovery/overview/#backup-on-demand","title":"Backup on Demand","text":"<p>To create a single backup of the <code>kcm</code>, a <code>ManagementBackup</code> object can be created manually, e.g. via the <code>kubectl</code> CLI. The object then creates only one instance of backup.</p> <p>Example</p> <p>An example of a <code>ManagementBackup</code> object:</p> <pre><code> apiVersion: k0rdent.mirantis.com/v1alpha1\n kind: ManagementBackup\n metadata:\n   name: example-backup\n spec:\n   storageLocation: my-location\n</code></pre>"},{"location":"disaster-recovery/overview/#whats-included-in-the-backup","title":"What's Included in the Backup","text":"<p>The backup includes all of the <code>kcm</code> component resources, parts of the <code>cert-manager</code> components required for other components creation, and all the required resources of <code>CAPI</code> and <code>ClusterDeployment</code>s currently in use in the management cluster.</p> <p>Example</p> <p>An example set of labels, and objects satisfying these labels will be included in the backup:</p> <pre><code> cluster.x-k8s.io/cluster-name=\"cluster-deployment-name\"\n cluster.x-k8s.io/provider=\"bootstrap-k0sproject-k0smotron\"\n cluster.x-k8s.io/provider=\"cluster-api\"\n cluster.x-k8s.io/provider=\"control-plane-k0sproject-k0smotron\"\n cluster.x-k8s.io/provider=\"infrastructure-aws\"\n controller.cert-manager.io/fao=\"true\"\n helm.toolkit.fluxcd.io/name=\"cluster-deployment-name\"\n k0rdent.mirantis.com/component=\"kcm\"\n</code></pre>"},{"location":"disaster-recovery/overview/#restoration","title":"Restoration","text":"<p>Note</p> <p>Caveats and limitations</p> <p>Please refer to the official migration documentation to be familiarized with potential limitations.</p> <p>To restore from a backup in the event of a disaster, the following actions should be performed:</p> <ol> <li> <p>Ensure a \u0441lean <code>kcm</code> installation. For more information, please refer    to the installation guide.</p> <p>For example:</p> <pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm \\\n --version &lt;version&gt; \\\n --create-namespace \\\n --namespace kcm-system \\\n --set controller.createManagement=false \\\n --set controller.createAccessManagement=false \\\n --set controller.createRelease=false \\\n --set controller.createTemplates=false\n</code></pre> </li> <li> <p>Create the <code>BackupStorageLocation</code>/<code>Secret</code> objects that had been    created during the preparation stage (preferably the same depending on a plugin).</p> </li> <li> <p>Restore the <code>kcm</code> system: either use the velero CLI    or directly create <code>Restore</code> object.</p> <p>Example</p> <p>A <code>Restore</code> object:</p> <pre><code> apiVersion: velero.io/v1\n kind: Restore\n metadata:\n   name: &lt;restore-name&gt;\n   namespace: kcm-system\n spec:\n   backupName: &lt;backup-name&gt;\n   excludedResources:\n   # the following are velero defaults, so it is recommended to keep them\n   - nodes\n   - events\n   - events.events.k8s.io\n   - backups.velero.io\n   - restores.velero.io\n   - resticrepositories.velero.io\n   - csinodes.storage.k8s.io\n   - volumeattachments.storage.k8s.io\n   - backuprepositories.velero.io\n   existingResourcePolicy: update\n   includedNamespaces:\n   - '*'\n</code></pre> <p>Example</p> <p>A <code>velero</code> CLI command:</p> <pre><code> velero --namespace kcm-system restore create &lt;restore-name&gt; --existing-resource-policy update --from-backup &lt;backup-name&gt;\n</code></pre> </li> <li> <p>Wait until the <code>Restore</code> status is <code>Completed</code> and all <code>kcm</code> components are up and running.</p> </li> </ol>"},{"location":"disaster-recovery/overview/#caveats-limitation","title":"Caveats / Limitation","text":"<p>All <code>velero</code> caveats and limitations are transitively implied in the <code>k0rdent</code>.</p> <p>In particular, that means no backup encryption is provided until it is implemented by a <code>velero</code> plugin that supports both encryption and cloud storage backups.</p>"},{"location":"quick-start/aws/","title":"AWS Quick Start","text":"<p>Much of the following includes the process of setting up credentials for AWS. To better understand how k0rdent uses credentials, read the Credential system.</p>"},{"location":"quick-start/aws/#prerequisites","title":"Prerequisites","text":""},{"location":"quick-start/aws/#k0rdent-management-cluster","title":"k0rdent Management Cluster","text":"<p>You need a Kubernetes cluster with kcm installed.</p>"},{"location":"quick-start/aws/#software-prerequisites","title":"Software prerequisites","text":"<p>The AWS <code>clusterawsadm</code> tool is required to bootstrap an AWS Account. Install it by following the AWS clusterawsadm installation instructions.</p>"},{"location":"quick-start/aws/#eks-deployment","title":"EKS Deployment","text":"<ul> <li>Additional EKS steps and verifications are described in EKS clusters.</li> </ul>"},{"location":"quick-start/aws/#configure-aws-iam","title":"Configure AWS IAM","text":"<p>Before launching a cluster on AWS, you need to set up your AWS infrastructure with the necessary IAM policies and service account.</p> <p>Note</p> <p> Skip steps below if you've already configured IAM policy for your AWS account</p> <ol> <li> <p>To use <code>clusterawsadm</code>, you must have an administrative user in an AWS    account. Once you have that administrator user, set your environment    variables:</p> <pre><code>export AWS_REGION=&lt;aws-region&gt;\nexport AWS_ACCESS_KEY_ID=&lt;admin-user-access-key&gt;\nexport AWS_SECRET_ACCESS_KEY=&lt;admin-user-secret-access-key&gt;\n# AWS_SESSION_TOKEN is optional when using Multi-Factor Auth.\nexport AWS_SESSION_TOKEN=&lt;session-token&gt;\n</code></pre> </li> <li> <p>After these are set, run this command to create the IAM CloudFormation stack:</p> <pre><code>clusterawsadm bootstrap iam create-cloudformation-stack\n</code></pre> </li> </ol>"},{"location":"quick-start/aws/#step-1-create-aws-iam-user","title":"Step 1: Create AWS IAM User","text":"<ol> <li> <p>Create an AWS IAM user with the following policies assigned:</p> <ul> <li><code>control-plane.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>controllers.cluster-api-provider-aws.sigs.k8s.io</code></li> <li><code>nodes.cluster-api-provider-aws.sigs.k8s.io</code></li> </ul> </li> <li> <p>Create Access Key for the IAM user.</p> <p>In the AWS IAM Console, create the Access Key for the IAM user and download its items (ID and Secret).</p> <p>You should have an <code>AccessKeyID</code> and a <code>SecretAccessKey</code> that look like the following:</p> Access key ID Secret access key AKIAQF+EXAMPLE EdJfFar6+example </li> </ol>"},{"location":"quick-start/aws/#step-2-create-the-iam-credentials-secret-on-k0rdent-management-cluster","title":"Step 2: Create the IAM Credentials Secret on k0rdent Management Cluster","text":"<p>Save the <code>Secret</code> YAML to a file named <code>aws-cluster-identity-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: aws-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\ntype: Opaque\nstringData:\n  AccessKeyID: AKIAQF+EXAMPLE\n  SecretAccessKey: EdJfFar6+example\n  # SessionToken is optional when using Multi-Factor Auth.\n  # SessionToken: IQoJb3JpZ2luX2VjEK7//+example\n</code></pre> <p>Apply the YAML to your cluster using the following command:</p> <pre><code>kubectl apply -f aws-cluster-identity-secret.yaml\n</code></pre> <p>Warning</p> <p> The secret must be created in the same <code>Namespace</code> where the CAPA provider is running. In case of k0rdent it's currently <code>kcm-system</code>. Placing secret in any other <code>Namespace</code> will result in the controller not able to read it.</p>"},{"location":"quick-start/aws/#step-3-create-awsclusterstaticidentity-object","title":"Step 3: Create AWSClusterStaticIdentity Object","text":"<p>Save the <code>AWSClusterStaticIdentity</code> YAML into a file named <code>aws-cluster-identity.yaml</code>:</p> <p>Note</p> <p> <code>.spec.secretRef</code> must match <code>.metadata.name</code> of the secret that was created in the previous step.</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\nkind: AWSClusterStaticIdentity\nmetadata:\n  name: aws-cluster-identity\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nspec:\n  secretRef: aws-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f aws-cluster-identity.yaml\n</code></pre>"},{"location":"quick-start/aws/#step-4-create-the-kcm-credential-object","title":"Step 4: Create the kcm Credential Object","text":"<p>Create a YAML with the specification of your credential and save it as <code>aws-cluster-identity-cred.yaml</code>.</p> <p>Note</p> <p> <code>.spec.identityRef.kind</code> must be <code>AWSClusterStaticIdentity</code> and the <code>.spec.identityRef.name</code> must match the <code>.metadata.name</code> of the <code>AWSClusterStaticIdentity</code> object.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: aws-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  description: \"Credential Example\"\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2\n    kind: AWSClusterStaticIdentity\n    name: aws-cluster-identity\n    namespace: kcm-system\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f aws-cluster-identity-cred.yaml\n</code></pre>"},{"location":"quick-start/aws/#step-5-create-your-first-cluster-deployment","title":"Step 5: Create Your First Cluster Deployment","text":"<p>Create a YAML with the specification of your Cluster Deployment and save it as <code>my-aws-clusterdeployment1.yaml</code>.</p> <p>Here is an example of a <code>ClusterDeployment</code> YAML file:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-aws-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-5\n  credential: aws-cluster-identity-cred\n  config:\n    region: us-east-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre> <p>Warning</p> <p> Don't forget to set proper AWS Region. </p> <p>Note</p> <p> To see available versions for <code>AWS</code> template run <code>kubectl get clustertemplate -n kcm-system</code>. </p> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f my-aws-clusterdeployment1.yaml\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-aws-clusterdeployment1 --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig, like this:</p> <pre><code>kubectl -n kcm-system get secret my-aws-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-aws-clusterdeployment1-kubeconfig.kubeconfig\n</code></pre> <pre><code>KUBECONFIG=\"my-aws-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre>"},{"location":"quick-start/azure/","title":"Azure Quick Start","text":"<p>Much of the following includes the process of setting up credentials for Azure. To better understand how k0rdent uses credentials, read the Credential System.</p>"},{"location":"quick-start/azure/#prerequisites","title":"Prerequisites","text":""},{"location":"quick-start/azure/#k0rdent-management-cluster","title":"k0rdent Management Cluster","text":"<p>You need a Kubernetes cluster with kcm installed.</p>"},{"location":"quick-start/azure/#software-prerequisites","title":"Software prerequisites","text":"<p>Before deploying Kubernetes clusters on Azure using k0rdent, ensure you have:</p> <p>The Azure CLI (<code>az</code>) is required to interact with Azure resources. Install it by following the Azure CLI installation instructions.</p> <p>Run the <code>az login</code> command to authenticate your session with Azure.</p>"},{"location":"quick-start/azure/#register-resource-providers","title":"Register resource providers","text":"<p>If you're using a new subscription that has never been used to deploy kcm or CAPI clusters, ensure the following resource providers are registered:</p> <ul> <li><code>Microsoft.Compute</code></li> <li><code>Microsoft.Network</code></li> <li><code>Microsoft.ContainerService</code></li> <li><code>Microsoft.ManagedIdentity</code></li> <li><code>Microsoft.Authorization</code></li> </ul> <p>To register these providers, run the following commands in the Azure CLI:</p> <pre><code>az provider register --namespace Microsoft.Compute\naz provider register --namespace Microsoft.Network\naz provider register --namespace Microsoft.ContainerService\naz provider register --namespace Microsoft.ManagedIdentity\naz provider register --namespace Microsoft.Authorization\n</code></pre> <p>You can follow the official documentation guide to register the providers.</p> <p>Before creating a cluster on Azure, set up credentials. This involves creating an <code>AzureClusterIdentity</code> and a Service Principal (SP) to let CAPZ (Cluster API Azure) communicate with Azure.</p>"},{"location":"quick-start/azure/#step-1-find-your-subscription-id","title":"Step 1: Find Your Subscription ID","text":"<p>List all your Azure subscriptions:</p> <pre><code>az account list -o table\n</code></pre> <p>Look for the Subscription ID of the account you want to use.</p> <p>Example output:</p> <pre><code>Name                     SubscriptionId                        TenantId\n-----------------------  -------------------------------------  --------------------------------\nMy Azure Subscription    12345678-1234-5678-1234-567812345678  87654321-1234-5678-1234-12345678\n</code></pre> <p>Copy your chosen Subscription ID for the next step.</p>"},{"location":"quick-start/azure/#step-2-create-a-service-principal-sp","title":"Step 2: Create a Service Principal (SP)","text":"<p>The Service Principal is like a password-protected user that CAPZ will use to manage resources on Azure.</p> <p>In your terminal, run the following command. Replace <code>&lt;subscription-id&gt;</code> with the ID you copied earlier:</p> <pre><code>az ad sp create-for-rbac --role contributor --scopes=\"/subscriptions/&lt;subscription-id&gt;\"\n</code></pre> <p>You will see output like this:</p> <pre><code>{\n \"appId\": \"12345678-7848-4ce6-9be9-a4b3eecca0ff\",\n \"displayName\": \"azure-cli-2024-10-24-17-36-47\",\n \"password\": \"12~34~I5zKrL5Kem2aXsXUw6tIig0M~3~1234567\",\n \"tenant\": \"12345678-959b-481f-b094-eb043a87570a\"\n}\n</code></pre> <p>Note</p> <p> Make sure to treat these strings like passwords. Do not share them or check them into a repository.</p>"},{"location":"quick-start/azure/#step-3-create-a-secret-object-with-the-azure-credentials","title":"Step 3: Create a Secret Object with the Azure credentials","text":"<p>For self-managed Azure clusters (non-AKS) create a Secret object that stores the <code>clientSecret</code> (password) from the Service Principal:</p> Azure cluster identity secret for non-AKS clusters <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: azure-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nstringData:\n  clientSecret: &lt;password&gt; # Password retrieved from the Service Principal\ntype: Opaque\n</code></pre> <p>For managed (AKS) clusters on Azure create the secret with the <code>AZURE_CLIENT_ID</code>, <code>AZURE_CLIENT_SECRET</code>, <code>AZURE_SUBSCRIPTION_ID</code> and <code>AZURE_TENANT_ID</code> keys set:</p> Credential secret for AKS clusters <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: azure-aks-credential\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nstringData:\n  AZURE_CLIENT_ID: &lt;app-id&gt; # AppId retrieved from the Service Principal\n  AZURE_CLIENT_SECRET: &lt;password&gt; # Password retrieved from the Service Principal\n  AZURE_SUBSCRIPTION_ID: &lt;subscription-id&gt; # The ID of the Subscription\n  AZURE_TENANT_ID: &lt;tenant-id&gt; # TenantID retrieved from the Service Principal\ntype: Opaque\n</code></pre> <p>Save the Secret YAML into a file named <code>azure-credentials-secret.yaml</code> and apply the YAML to your cluster using the following command:</p> <pre><code>kubectl apply -f azure-credentials-secret.yaml\n</code></pre>"},{"location":"quick-start/azure/#step-4-create-the-azureclusteridentity-object","title":"Step 4: Create the AzureClusterIdentity Object","text":"<p>Info</p> <p> Skip this step for managed (AKS) clusters.</p> <p>This object defines the credentials CAPZ will use to manage Azure resources. It references the Secret you just created above.</p> <p>Warning</p> <p> Make sure that <code>.spec.clientSecret.name</code> matches the name of the Secret you created in the previous step.</p> <p>Save the following YAML into a file named <code>azure-cluster-identity.yaml</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: AzureClusterIdentity\nmetadata:\n  labels:\n    clusterctl.cluster.x-k8s.io/move-hierarchy: \"true\"\n    k0rdent.mirantis.com/component: \"kcm\"\n  name: azure-cluster-identity\n  namespace: kcm-system\nspec:\n  allowedNamespaces: {}\n  clientID: &lt;appId&gt; # The App ID retrieved from the Service Principal above in Step 2\n  clientSecret:\n    name: azure-cluster-identity-secret\n    namespace: kcm-system\n  tenantID: &lt;tenant&gt; # The Tenant ID retrieved from the Service Principal above in Step 2\n  type: ServicePrincipal\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-cluster-identity.yaml\n</code></pre>"},{"location":"quick-start/azure/#step-5-create-the-kcm-credential-object","title":"Step 5: Create the kcm Credential Object","text":"<p>Create a YAML with the specification of our credential and save it as <code>azure-credential.yaml</code>.</p> <p>Warning</p> <ol> <li>For non-AKS clusters, the <code>.spec.identityRef.kind</code> must be set to <code>AzureClusterIdentity</code>, and <code>.spec.name</code> must match <code>.metadata.name</code> of the <code>AzureClusterIdentity</code> object created in the previous step.</li> <li>For AKS clusters, the <code>.spec.identityRef.kind</code> must be set to <code>Secret</code>, and <code>.spec.name</code> must match <code>.metadata.name</code> of the <code>Secret</code> object created in Step 3.</li> </ol> Credential object for non-AKS clusters <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: azure-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: AzureClusterIdentity\n    name: azure-cluster-identity\n    namespace: kcm-system\n</code></pre> Credential object for AKS clusters <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: azure-aks-credential\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: v1\n    kind: Secret\n    name: azure-aks-credential\n    namespace: kcm-system\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f azure-credential.yaml\n</code></pre> <p>This creates the <code>Credential</code> object that will be used in the next step.</p>"},{"location":"quick-start/azure/#step-6-create-your-first-clusterdeployment","title":"Step 6: Create your first ClusterDeployment","text":"<p>Create a YAML with the specification of your ClusterDeployment and save it as <code>my-azure-clusterdeployment1.yaml</code>.</p> <p>Here is the examples of a <code>ClusterDeployment</code> YAML file:</p> Example of a self-managed (non-AKS) Azure ClusterDeployment <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-azure-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: azure-standalone-cp-0-0-5\n  credential: azure-cluster-identity-cred\n  config:\n    location: \"westus\" # Select your desired Azure Location (find it via `az account list-locations -o table`)\n    subscriptionID: &lt;subscription-id&gt; # Enter the Subscription ID used earlier\n    controlPlane:\n      vmSize: Standard_A4_v2\n    worker:\n      vmSize: Standard_A4_v2\n</code></pre> Example of the AKS ClusterDeployment <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-azure-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: azure-aks-0-0-2\n  credential: azure-aks-credential\n  propagateCredentials: false # Should be set to `false`\n  config:\n    location: \"westus\" # Select your desired Azure Location (find it via `az account list-locations -o table`)\n    machinePools:\n      system:\n        vmSize: Standard_A4_v2\n      user:\n        vmSize: Standard_A4_v2\n</code></pre> <p>Note</p> <p> To see available versions for <code>Azure</code> template run <code>kubectl get clustertemplate -n kcm-system</code>. </p> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f my-azure-clusterdeployment1.yaml\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-azure-clusterdeployment1 --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig, like this:</p> <pre><code>kubectl -n kcm-system get secret my-azure-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-azure-clusterdeployment1-kubeconfig.kubeconfig\n</code></pre> <pre><code>KUBECONFIG=\"my-azure-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre>"},{"location":"quick-start/installation/","title":"Installation","text":""},{"location":"quick-start/installation/#requirements","title":"Requirements","text":"<p>k0rdent requires a Kubernetes cluster. It can be of any type and will become the k0rdent management cluster.</p> <p>If you don't have a Kubernetes cluster yet, consider using k0s.</p> <p>The following instructions assume:</p> <ul> <li>Your <code>kubeconfig</code> points to the correct Kubernetes cluster.</li> <li>You have Helm installed.</li> <li>You have kubectl installed.</li> </ul>"},{"location":"quick-start/installation/#helpful-tools","title":"Helpful Tools","text":"<p>It may be helpful to have the following tools installed:</p> <ul> <li>clusterctl</li> <li>Mirantis Lens</li> <li>k9s</li> </ul>"},{"location":"quick-start/installation/#installation-via-helm","title":"Installation via Helm","text":"<pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm --version 0.0.7 -n kcm-system --create-namespace\n</code></pre>"},{"location":"quick-start/installation/#verification","title":"Verification","text":"<p>The installation takes a few minutes until kcm and its subcomponents are fully installed and configured.</p>"},{"location":"quick-start/installation/#verify-core-components-are-running","title":"Verify Core Components are running","text":"<p>Check pods are running in the <code>kcm-system</code> namespace with the following command:</p> <pre><code>kubectl get pods -n kcm-system\n</code></pre> <p>The output should be similar to: <pre><code>NAME                                                           READY   STATUS\nazureserviceoperator-controller-manager-86d566cdbc-rqkt9       1/1     Running\ncapa-controller-manager-7cd699df45-28hth                       1/1     Running\ncapi-controller-manager-6bc5fc5f88-hd8pv                       1/1     Running\ncapv-controller-manager-bb5ff9bd5-7dsr9                        1/1     Running\ncapz-controller-manager-5dd988768-qjdbl                        1/1     Running\nhelm-controller-76f675f6b7-4d47l                               1/1     Running\nkcm-cert-manager-7c8bd964b4-nhxnq                              1/1     Running\nkcm-cert-manager-cainjector-56476c46f9-xvqhh                   1/1     Running\nkcm-cert-manager-webhook-69d7fccf68-s46w8                      1/1     Running\nkcm-cluster-api-operator-79459d8575-2s9jc                      1/1     Running\nkcm-controller-manager-64869d9f9d-zktgw                        1/1     Running\nk0smotron-controller-manager-bootstrap-6c5f6c7884-d2fqs        2/2     Running\nk0smotron-controller-manager-control-plane-857b8bffd4-zxkx2    2/2     Running\nk0smotron-controller-manager-infrastructure-7f77f55675-tv8vb   2/2     Running\nsource-controller-5f648d6f5d-7mhz5                             1/1     Running\n</code></pre></p> <p>Checking pods are running in the <code>projectsveltos</code> namespace with the following command: <pre><code>kubectl get pods -n projectsveltos\n</code></pre></p> <p>The output should be similar to: <pre><code>NAME                                     READY   STATUS    RESTARTS   AGE\naccess-manager-cd49cffc9-c4q97           1/1     Running   0          16m\naddon-controller-64c7f69796-whw25        1/1     Running   0          16m\nclassifier-manager-574c9d794d-j8852      1/1     Running   0          16m\nconversion-webhook-5d78b6c648-p6pxd      1/1     Running   0          16m\nevent-manager-6df545b4d7-mbjh5           1/1     Running   0          16m\nhc-manager-7b749c57d-5phkb               1/1     Running   0          16m\nsc-manager-f5797c4f8-ptmvh               1/1     Running   0          16m\nshard-controller-767975966-v5qqn         1/1     Running   0          16m\nsveltos-agent-manager-56bbf5fb94-9lskd   1/1     Running   0          15m\n</code></pre></p> <p>If you have fewer pods, give kcm more time to reconcile all the pods.</p>"},{"location":"quick-start/installation/#verify-kcm-templates-have-been-successfully-reconciled","title":"Verify kcm templates have been successfully reconciled","text":"<p>For additional verification, check that the example templates packaged with kcm have been installed and are valid.</p> <p>Check <code>ProviderTemplate</code> objects with:</p> <pre><code>kubectl get providertemplate -n kcm-system\n</code></pre> <p>The output should be similar to:</p> <pre><code>NAME                                 VALID\ncluster-api-X-Y-Z                    true\ncluster-api-provider-aws-X-Y-Z       true\ncluster-api-provider-azure-X-Y-Z     true\ncluster-api-provider-vsphere-X-Y-Z   true\nkcm-X-Y-Z                            true\nk0smotron-X-Y-Z                      true\nprojectsveltos-X-Y-Z                 true\n</code></pre> <p>Check <code>ClusterTemplate</code> objects with:</p> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>The output should be similar to:</p> <pre><code>NAME                                VALID\naws-eks-X-Y-Z                       true\naws-hosted-cp-X-Y-Z                 true\naws-standalone-cp-X-Y-Z             true\nazure-hosted-cp-X-Y-Z               true\nazure-standalone-cp-X-Y-Z           true\nvsphere-hosted-cp-X-Y-Z             true\nvsphere-standalone-cp-X-Y-Z         true\n</code></pre> <p>Check <code>ServiceTemplate</code> objects with:</p> <pre><code>kubectl get servicetemplate -n kcm-system\n</code></pre> <p>The output should be similar to:</p> <pre><code>NAME                  VALID\ningress-nginx-X-Y-Z   true\ningress-nginx-X-Y-Z   true\nkyverno-X-Y-Z         true\n</code></pre>"},{"location":"quick-start/installation/#next-step","title":"Next Step","text":"<p>Now you can configure your Infrastructure Provider of choice and create your first Managed Cluster.</p> <p>Jump to any of the following Infrastructure Providers for specific instructions:</p> <ul> <li>AWS Quick Start</li> <li>Azure Quick Start</li> <li>vSphere Quick Start</li> <li>OpenStack Quick Start</li> </ul>"},{"location":"quick-start/openstack/","title":"OpenStack Quick Start","text":"<p>Much of the following includes the process of setting up credentials for OpenStack. To better understand how k0rdent uses credentials, read the Credential system.</p>"},{"location":"quick-start/openstack/#prerequisites","title":"Prerequisites","text":""},{"location":"quick-start/openstack/#k0rdent-management-cluster","title":"k0rdent Management Cluster","text":"<p>You need a Kubernetes cluster with kcm installed.</p>"},{"location":"quick-start/openstack/#software-prerequisites","title":"Software prerequisites","text":"<ul> <li>OpenStack CLI (optional)</li> <li>Application Credential in OpenStack (recommended for enhanced security)     If you prefer username/password, adjust accordingly in your YAML.</li> </ul>"},{"location":"quick-start/openstack/#step-1-configure-openstack-application-credential","title":"Step 1: Configure OpenStack Application Credential","text":"<p>This credential should include:</p> <ul> <li>OS_AUTH_URL</li> <li>OS_APPLICATION_CREDENTIAL_ID</li> <li>OS_APPLICATION_CREDENTIAL_SECRET</li> <li>OS_REGION_NAME</li> <li>OS_INTERFACE</li> <li>OS_IDENTITY_API_VERSION (commonly 3)</li> <li>OS_AUTH_TYPE (e.g., v3applicationcredential)</li> </ul> <p>Note</p> <p> Using an Application Credential is strongly recommended because it limits scope and improves security over a raw username/password approach.</p>"},{"location":"quick-start/openstack/#step-2-create-the-openstack-credentials-secret-on-k0rdent-management-cluster","title":"Step 2: Create the OpenStack Credentials Secret on k0rdent Management Cluster","text":"<p>Create a Kubernetes Secret containing the clouds.yaml that defines your OpenStack environment. Save this as <code>openstack-cloud-config.yaml</code> (for example):</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: openstack-cloud-config\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nstringData:\n  clouds.yaml: |\n    clouds:\n      openstack:\n        auth:\n          auth_url: &lt;OS_AUTH_URL&gt;\n          application_credential_id: &lt;OS_APPLICATION_CREDENTIAL_ID&gt;\n          application_credential_secret: &lt;OS_APPLICATION_CREDENTIAL_SECRET&gt;\n        region_name: &lt;OS_REGION_NAME&gt;\n        interface: &lt;OS_INTERFACE&gt;\n        identity_api_version: &lt;OS_IDENTITY_API_VERSION&gt;\n        auth_type: &lt;OS_AUTH_TYPE&gt;\n</code></pre> <p>Apply the YAML to your cluster using the following command:</p> <pre><code>kubectl apply -f openstack-cloud-config.yaml\n</code></pre>"},{"location":"quick-start/openstack/#step-3-create-the-k0rdent-credential-object","title":"Step 3: Create the k0rdent Credential Object","text":"<p>Next, define a Credential that references the Secret from Step 2. Save this as <code>openstack-cluster-identity-cred.yaml</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: openstack-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  description: \"OpenStack credentials\"\n  identityRef:\n    apiVersion: v1\n    kind: Secret\n    name: openstack-cloud-config\n    namespace: kcm-system\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f openstack-cluster-identity-cred.yaml\n</code></pre> <p>Note</p> <ol> <li>.spec.identityRef.kind hould be Secret.</li> <li>.spec.identityRef.name must match the Secret you created in Step 2.</li> <li>.spec.identityRef.namespace must be the same as the Secret\u2019s namespace (kcm-system).</li> </ol>"},{"location":"quick-start/openstack/#step-4-create-your-first-managed-cluster","title":"Step 4: Create Your First Managed Cluster","text":"<p>Create a YAML with the specification of your Managed Cluster and save it as <code>my-openstack-cluster-deployment.yaml</code>.</p> <p>Here is an example:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-openstack-cluster-deployment\n  namespace: kcm-system\nspec:\n  template: openstack-standalone-cp-0-0-2\n  credential: openstack-cluster-identity-cred\n  config:\n    controlPlaneNumber: 1\n    workersNumber: 1\n    controlPlane:\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    worker:\n      flavor: m1.medium\n      image:\n        filter:\n          name: ubuntu-22.04-x86_64\n    authURL: &lt;OS_AUTH_URL&gt;\n</code></pre> <p>Note</p> <ol> <li>spec.template references the OpenStack-specific blueprint (e.g., openstack-standalone-cp-0-0-1).</li> <li>Adjust flavor, image name, and authURL to match your OpenStack environment.</li> <li>For more information about the config options, see the OpenStack Template Parameters.</li> <li>To see available versions for <code>OpenStack</code> template run <code>kubectl get clustertemplate -n kcm-system</code>.</li> </ol> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f my-openstack-cluster-deployment.yaml\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-openstack-cluster-deployment --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig, like this:</p> <pre><code>kubectl -n kcm-system get secret my-openstack-cluster-deployment-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-openstack-cluster-deployment-kubeconfig.kubeconfig\n</code></pre> <pre><code>KUBECONFIG=\"my-openstack-cluster-deployment-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre>"},{"location":"quick-start/vsphere/","title":"vSphere Quick Start","text":"<p>Much of the following includes the process of setting up credentials for vSphere. To better understand how k0rdent uses credentials, read the Credential System.</p>"},{"location":"quick-start/vsphere/#prerequisites","title":"Prerequisites","text":""},{"location":"quick-start/vsphere/#k0rdent-management-cluster","title":"k0rdent Management Cluster","text":"<p>You need a Kubernetes cluster with kcm installed.</p>"},{"location":"quick-start/vsphere/#software-vmware-specific-prerequisites","title":"Software &amp; VMware-specific prerequisites","text":"<ol> <li><code>kubectl</code> CLI installed locally.</li> <li>vSphere instance version <code>6.7.0</code> or higher.</li> <li>vSphere account with appropriate privileges.</li> <li>Image template.</li> <li>vSphere network with DHCP enabled.</li> </ol>"},{"location":"quick-start/vsphere/#vsphere-privileges","title":"vSphere privileges","text":"<p>To function properly, the user assigned to the vSphere Provider should be able to manipulate vSphere resources. The following is the general overview of the required privileges:</p> <ul> <li><code>Virtual machine</code> - full permissions are required</li> <li><code>Network</code> - <code>Assign network</code> is sufficient</li> <li><code>Datastore</code> - it should be possible for user to manipulate virtual machine   files and metadata</li> </ul> <p>In addition to that, specific CSI driver permissions are required. See the official doc for more information on CSI-specific permissions.</p>"},{"location":"quick-start/vsphere/#image-template","title":"Image template","text":"<p>You can use pre-built image templates from the CAPV project or build your own.</p> <p>When building your own image, make sure that VMware tools and cloud-init are installed and properly configured.</p> <p>You can follow the official open-vm-tools guide on how to correctly install VMware tools.</p> <p>When setting up cloud-init, you can refer to the official docs and specifically the VMware datasource docs for extended information regarding cloud-init on vSphere.</p>"},{"location":"quick-start/vsphere/#vsphere-network","title":"vSphere network","text":"<p>When creating a network, make sure that it has DHCP service.</p> <p>Also, ensure that part of your network is out of the DHCP range (e.g., network <code>172.16.0.0/24</code> should have DHCP range <code>172.16.0.100-172.16.0.254</code> only). This is needed to ensure that LB services will not create any IP conflicts in the network.</p>"},{"location":"quick-start/vsphere/#step-1-create-a-secret-object-with-the-username-and-password","title":"Step 1: Create a Secret Object with the username and password","text":"<p>The Secret stores the username and password for your vSphere instance.</p> <p>Save the Secret YAML into a file named <code>vsphere-cluster-identity-secret.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: vsphere-cluster-identity-secret\n  namespace: kcm-system\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nstringData:\n  username: &lt;user&gt;\n  password: &lt;password&gt;\ntype: Opaque\n</code></pre> <p>Apply the YAML to your cluster using the following command:</p> <pre><code>kubectl apply -f vsphere-cluster-identity-secret.yaml\n</code></pre>"},{"location":"quick-start/vsphere/#step-2-create-the-vsphereclusteridentity-object","title":"Step 2: Create the VSphereClusterIdentity Object","text":"<p>This object defines the credentials CAPV will use to manage vSphere resources.</p> <p>Save the VSphereClusterIdentity YAML into a file named <code>vsphere-cluster-identity.yaml</code>:</p> <pre><code>apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\nkind: VSphereClusterIdentity\nmetadata:\n  name: vsphere-cluster-identity\n  labels:\n    k0rdent.mirantis.com/component: \"kcm\"\nspec:\n  secretName: vsphere-cluster-identity-secret\n  allowedNamespaces:\n    selector:\n      matchLabels: {}\n</code></pre> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity.yaml\n</code></pre>"},{"location":"quick-start/vsphere/#step-3-create-the-kcm-credential-object","title":"Step 3: Create the kcm Credential Object","text":"<p>Create a YAML with the specification of our credential and save it as <code>vsphere-cluster-identity-cred.yaml</code></p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Credential\nmetadata:\n  name: vsphere-cluster-identity-cred\n  namespace: kcm-system\nspec:\n  identityRef:\n    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n    kind: VSphereClusterIdentity\n    name: vsphere-cluster-identity\n</code></pre> <p>Warning</p> <p> <code>.spec.identityRef.kind</code> must be <code>VSphereClusterIdentity</code> and the <code>.spec.identityRef.name</code> must match the <code>.metadata.name</code> of the <code>VSphereClusterIdentity</code> object above.</p> <p>Apply the YAML to your cluster:</p> <pre><code>kubectl apply -f vsphere-cluster-identity-cred.yaml\n</code></pre>"},{"location":"quick-start/vsphere/#step-4-create-your-first-cluster-deployment","title":"Step 4: Create your first Cluster Deployment","text":"<p>Create a YAML with the specification of your Cluster Deployment and save it as <code>my-vsphere-clusterdeployment1.yaml</code>.</p> <p>Here is an example of a <code>ClusterDeployment</code> YAML file:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-vsphere-clusterdeployment1\n  namespace: kcm-system\nspec:\n  template: vsphere-standalone-cp-0-0-5\n  credential: vsphere-cluster-identity-cred\n  config:\n    vsphere:\n      server: &lt;VSPHERE_SERVER&gt;\n      thumbprint: &lt;VSPHERE_THUMBPRINT&gt;\n      datacenter: &lt;VSPHERE_DATACENTER&gt;\n      datastore: &lt;VSPHERE_DATASTORE&gt;\n      resourcePool: &lt;VSPHERE_RESOURCEPOOL&gt;\n      folder: &lt;VSPHERE_FOLDER&gt;\n    controlPlaneEndpointIP: &lt;VSPHERE_CONTROL_PLANE_ENDPOINT&gt;\n    controlPlane:\n      ssh:\n        user: ubuntu\n        publicKey: &lt;VSPHERE_SSH_KEY&gt;\n      vmTemplate: &lt;VSPHERE_VM_TEMPLATE&gt;\n      network: &lt;VSPHERE_NETWORK&gt;\n    worker:\n      ssh:\n        user: ubuntu\n        publicKey: &lt;VSPHERE_SSH_KEY&gt;\n      vmTemplate: &lt;VSPHERE_VM_TEMPLATE&gt;\n      network: &lt;VSPHERE_NETWORK&gt;\n</code></pre> <p>Note</p> <p> To see available versions for <code>vSphere</code> template run <code>kubectl get clustertemplate -n kcm-system</code>.</p> <p>For more information about the config options, see the vSphere Template Parameters.</p> <p>Apply the YAML to your management cluster:</p> <pre><code>kubectl apply -f my-vsphere-clusterdeployment1.yaml\n</code></pre> <p>There will be a delay as the cluster finishes provisioning. Follow the provisioning process with the following command:</p> <pre><code>kubectl -n kcm-system get clusterdeployment.k0rdent.mirantis.com my-vsphere-clusterdeployment1 --watch\n</code></pre> <p>After the cluster is <code>Ready</code>, you can access it via the kubeconfig, like this:</p> <pre><code>kubectl -n kcm-system get secret my-vsphere-clusterdeployment1-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt; my-vsphere-clusterdeployment1-kubeconfig.kubeconfig\n</code></pre> <pre><code>KUBECONFIG=\"my-vsphere-clusterdeployment1-kubeconfig.kubeconfig\" kubectl get pods -A\n</code></pre> <p>To delete provisioned cluster and free consumed vSphere resources run:</p> <pre><code>kubectl -n kcm-system delete cluster my-vsphere-clusterdeployment1\n</code></pre>"},{"location":"rbac/roles/","title":"k0rdent Role-Based Access Control","text":"<p>k0rdent leverages the Kubernetes RBAC system and provides a set of standard <code>ClusterRoles</code> with associated permissions. All <code>ClusterRoles</code> are created as part of the kcm helm chart. k0rdent roles are based on labels and aggregated permissions, meaning they automatically collect rules from other <code>ClusterRoles</code> with specific labels.</p> <p>The following table outlines the roles available in k0rdent, along with their respective read/write or read-only permissions:</p> Roles Global Admin Global Viewer Namespace Admin Namespace Editor Namespace Viewer Scope Global Global Namespace Namespace Namespace k0rdent management r/w r/o - - - Namespaces management r/w r/o - - - Provider Templates r/w r/o - - - Global Template Management r/w r/o - - - Multi Cluster Service Management r/w r/o - - - Template Chain Management r/w r/o r/w r/o r/o Cluster and Service Templates r/w r/o r/w r/o r/o Credentials r/w r/o r/w r/o r/o Flux Helm objects r/w r/o r/w r/o r/o Cluster Deployments r/w r/o r/w r/w r/o"},{"location":"rbac/roles/#roles-definition","title":"Roles definition","text":"<p>This section provides an overview of all <code>ClusterRoles</code> available in k0rdent.</p> <p>Note</p> <p> The names of the <code>ClusterRoles</code> may have different prefix depending on the name of the kcm Helm chart. The <code>ClusterRoles</code> definitions below use the <code>kcm</code> prefix, which is the default name of the kcm Helm chart.</p>"},{"location":"rbac/roles/#global-admin","title":"Global Admin","text":"<p>The <code>Global Admin</code> role provides full administrative access across all the k0rdent system.</p> <p>Name: <code>kcm-global-admin-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRoles</code> with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-global-admin: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-admin: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-editor: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Full access to k0rdent API</li> <li>Full access to Flux Helm repositories and Helm charts</li> <li>Full access to Cluster API identities</li> <li>Full access to namespaces and secrets</li> </ol> <p>Use case</p> <p>A user with the <code>Global Admin</code> role is authorized to perform the following actions:</p> <ol> <li>Manage the k0rdent configuration</li> <li>Manage namespaces in the management cluster</li> <li>Manage <code>Provider Templates</code>: add new templates or remove unneeded ones</li> <li>Manage <code>Cluster</code> and <code>Service Templates</code> in any namespace, including adding and removing templates</li> <li>Manage Flux <code>HelmRepositories</code> and <code>HelmCharts</code> in any namespace</li> <li>Manage access rules for <code>Cluster</code> and <code>Service Templates</code>, including distributing templates across namespaces using    <code>Template Chains</code></li> <li>Manage upgrade sequences for <code>Cluster</code> and <code>Service Templates</code></li> <li>Manage and deploy Services across multiple clusters in any namespace by modifying <code>MultiClusterService</code> resources</li> <li>Manage <code>ClusterDeployments</code> in any namespace</li> <li>Manage <code>Credentials</code> and <code>secrets</code> in any namespace</li> <li>Upgrade k0rdent</li> <li>Uninstall k0rdent</li> </ol>"},{"location":"rbac/roles/#global-viewer","title":"Global Viewer","text":"<p>The <code>Global Viewer</code> role grants read-only access across the k0rdent system. It does not permit any modifications, including the creation of clusters.</p> <p>Name: <code>kcm-global-viewer-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRoles</code> with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-global-viewer: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-viewer: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Read access to k0rdent API</li> <li>Read access to Flux Helm repositories and Helm charts</li> <li>Read access to Cluster API identities</li> <li>Read access to namespaces and secrets</li> </ol> <p>Use case</p> <p>A user with the <code>Global Viewer</code> role is authorized to perform the following actions:</p> <ol> <li>View the k0rdent configuration</li> <li>List namespaces available in the management cluster</li> <li>List and get the detailed information about available <code>Provider Templates</code></li> <li>List available <code>Cluster</code> and <code>Service Templates</code> in any namespace</li> <li>List and view detailed information about Flux <code>HelmRepositories</code> and <code>HelmCharts</code> in any namespace</li> <li>View access rules for <code>Cluster</code> and <code>Service Templates</code>, including <code>Template Chains</code> in any namespace</li> <li>View full details about the created <code>MultiClusterService</code> objects</li> <li>List and view detailed information about <code>ClusterDeployments</code> in any namespace</li> <li>List and view detailed information about created <code>Credentials</code> and <code>secrets</code> in any namespace</li> </ol>"},{"location":"rbac/roles/#namespace-admin","title":"Namespace Admin","text":"<p>The <code>Namespace Admin</code> role provides full administrative access within namespace.</p> <p>Name: <code>kcm-namespace-admin-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRoles</code> with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-admin: true</code></li> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-editor: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Full access to <code>ClusterDeployments</code>, <code>Credentials</code>, <code>Cluster</code> and <code>Service Templates</code> in the namespace</li> <li>Full access to <code>Template Chains</code> in the namespace</li> <li>Full access to Flux <code>HelmRepositories</code> and <code>HelmCharts</code> in the namespace</li> </ol> <p>Use case</p> <p>A user with the <code>Namespace Admin</code> role is authorized to perform the following actions within the namespace:</p> <ol> <li>Create and manage all <code>ClusterDeployments</code> in the namespace</li> <li>Create and manage <code>Cluster</code> and <code>Service Templates</code> in the namespace</li> <li>Manage the distribution and upgrade sequences of Templates within the namespace</li> <li>Create and manage Flux <code>HelmRepositories</code> and <code>HelmCharts</code> in the namespace</li> <li>Manage <code>Credentials</code> created by any user in the namespace</li> </ol>"},{"location":"rbac/roles/#namespace-editor","title":"Namespace Editor","text":"<p>The <code>Namespace Editor</code> role allows users to create and modify <code>ClusterDeployments</code> within namespace using predefined <code>Credentials</code> and <code>Templates</code>.</p> <p>Name: <code>kcm-namespace-editor-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRoles</code> with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-editor: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Full access to <code>ClusterDeployments</code> in the allowed namespace</li> <li>Read access to <code>Credentials</code>, <code>Cluster</code> and <code>Service Templates</code>, and <code>TemplateChains</code> in the namespace</li> <li>Read access to Flux <code>HelmRepositories</code> and <code>HelmCharts</code> in the namespace</li> </ol> <p>Use case</p> <p>A user with the <code>Namespace Editor</code> role has the following permissions in the namespace:</p> <ol> <li>Can create and manage <code>ClusterDeployment</code> objects in the namespace using existing <code>Credentials</code> and <code>Templates</code></li> <li>Can list and view detailed information about the <code>Credentials</code> available in the namespace</li> <li>Can list and view detailed information about the available <code>Cluster</code> and <code>Service Templates</code> and the <code>Templates'</code>    upgrade sequences</li> <li>Can list and view detailed information about the Flux <code>HelmRepositories</code> and <code>HelmCharts</code></li> </ol>"},{"location":"rbac/roles/#namespace-viewer","title":"Namespace Viewer","text":"<p>The <code>Namespace Viewer</code> role grants read-only access to resources within a namespace.</p> <p>Name: <code>kcm-namespace-viewer-role</code></p> <p>Aggregation Rule: Includes all <code>ClusterRoles</code> with the labels:</p> <ul> <li><code>k0rdent.mirantis.com/aggregate-to-namespace-viewer: true</code></li> </ul> <p>Permissions:</p> <ol> <li>Read access to <code>ClusterDeployments</code> in the namespace</li> <li>Read access to <code>Credentials</code>, <code>Cluster</code> and <code>Service Templates</code>, and <code>TemplateChains</code> in the namespace</li> <li>Read access to Flux <code>HelmRepositories</code> and <code>HelmCharts</code> in the namespace</li> </ol> <p>Use case</p> <p>A user with the <code>Namespace Viewer</code> role has the following permissions in the namespace:</p> <ol> <li>Can list and view detailed information about all the <code>ClusterDeployment</code> objects in the allowed namespace</li> <li>Can list and view detailed information about <code>Credentials</code> available in the specific namespace</li> <li>Can list and view detailed information about available <code>Cluster</code> and <code>Service Templates</code> and the <code>Templates'</code>    upgrade sequences</li> <li>Can list and view detailed information about Flux <code>HelmRepositories</code> and <code>HelmCharts</code></li> </ol>"},{"location":"template/byo-templates/","title":"Bring your own Templates","text":"<p>This guide outlines the steps to bring your own Template to kcm.</p>"},{"location":"template/byo-templates/#create-a-source-object","title":"Create a Source Object","text":"<p>Info</p> <p> Skip this step if you're using an existing source.</p> <p>A source object defines where the Helm chart is stored. The source can be one of the following types: HelmRepository, GitRepository or Bucket.</p> <p>Notes</p> <ol> <li>The source object must exist in the same namespace as the Template.</li> <li>For cluster-scoped <code>ProviderTemplates</code>, the referenced source must reside in the system namespace (default: <code>kcm-system</code>).</li> </ol>"},{"location":"template/byo-templates/#example-custom-source-object-with-helmrepository-kind","title":"Example: Custom Source Object with HelmRepository Kind","text":"<pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: custom-templates-repo\n  namespace: kcm-system\nspec:\n  insecure: true\n  interval: 10m0s\n  provider: generic\n  type: oci\n  url: oci://ghcr.io/external-templates-repo/charts\n</code></pre>"},{"location":"template/byo-templates/#create-the-template","title":"Create the Template","text":"<p>Create a Template object of the desired type:</p> <ul> <li><code>ClusterTemplate</code></li> <li><code>ServiceTemplate</code></li> <li><code>ProviderTemplate</code></li> </ul> <p>For <code>ClusterTemplate</code> and <code>ServiceTemplate</code> configure the namespace where this template should reside (<code>metadata.namespace</code>). The custom Template requires a helm chart definition in the <code>.spec.helm.chartSpec</code> field of the HelmChartSpec kind or the reference to already existing <code>HelmChart</code> object in <code>.spec.helm.chartRef</code>.</p> <p>Note</p> <p> <code>spec.helm.chartSpec</code> and <code>spec.helm.chartRef</code> are mutually exclusive.</p> <p>To automatically create the <code>HelmChart</code> for the <code>Template</code>, configure the following custom helm chart parameters under <code>spec.helm.chartSpec</code>:</p> Field Description <code>sourceRef</code>LocalHelmChartSourceReference Reference to the source object (e.g., <code>HelmRepository</code>, <code>GitRepository</code>, or <code>Bucket</code>) in the same namespace as the Template. <code>chart</code>string The name of the Helm chart available in the source. <code>version</code>string Version is the chart version semver expression. Defaults to latest when omitted. <code>interval</code>Kubernetes meta/v1.Duration The frequency at which the <code>sourceRef</code> is checked for updates. Defaults to 10 minutes. <p>For the complete list of the <code>HelmChart</code> parameters, see: HelmChartSpec.</p> <p>The controller will automatically create the <code>HelmChart</code> object based on the chartSpec defined in <code>.spec.helm.chartSpec</code>.</p> <p>Note</p> <p> <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects should reside in the same namespace as the <code>ClusterDeployment</code> referencing them. The <code>ClusterDeployment</code> can't reference the Template from another namespace (the creation request will be declined by the admission webhook). All <code>ClusterTemplates</code> and <code>ServiceTemplates</code> shipped with kcm reside in the system namespace (defaults to <code>kcm-system</code>). To get the instructions on how to distribute Templates along multiple namespaces, read Template Life Cycle Management.</p>"},{"location":"template/byo-templates/#example-custom-clustertemplate-with-the-chart-definition-to-create-a-new-helmchart","title":"Example: Custom ClusterTemplate with the Chart Definition to Create a new HelmChart","text":"<pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterTemplate\nmetadata:\n  name: custom-template\n  namespace: kcm-system\nspec:\n  providers:\n    - bootstrap-k0sproject-k0smotron\n    - control-plane-k0sproject-k0smotron\n    - infrastructure-openstack\n  helm:\n    chartSpec:\n      chart: os-k0sproject-k0smotron\n      sourceRef:\n        kind: HelmRepository\n        name: custom-templates-repo\n</code></pre>"},{"location":"template/byo-templates/#example-custom-clustertemplate-referencing-an-existing-helmchart-object","title":"Example: Custom ClusterTemplate Referencing an Existing HelmChart object","text":"<pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterTemplate\nmetadata:\n  name: custom-template\n  namespace: kcm-system\nspec:\n  helm:\n    chartRef:\n      kind: HelmChart\n      name: custom-chart\n</code></pre>"},{"location":"template/byo-templates/#required-and-exposed-providers-definition","title":"Required and exposed providers definition","text":"<p>The <code>*Template</code> object must specify the list of Cluster API providers that are either required (for <code>ClusterTemplates</code> and <code>ServiceTemplates</code>) or exposed (for <code>ProviderTemplates</code>). These providers include <code>infrastructure</code>, <code>bootstrap</code>, and <code>control-plane</code>. This can be achieved in two ways:</p> <ol> <li>By listing the providers explicitly in the <code>spec.providers</code> field.</li> <li>Alternatively, by including specific annotations in the <code>Chart.yaml</code> of the referenced Helm chart. The annotations should list the providers as a <code>comma-separated</code> value.</li> </ol> <p>For example:</p> <p><code>Template</code> spec:</p> <pre><code>spec:\n  providers:\n  - bootstrap-k0sproject-k0smotron\n  - control-plane-k0sproject-k0smotron\n  - infrastructure-aws\n</code></pre> <p><code>Chart.yaml</code>:</p> <pre><code>annotations:\n  cluster.x-k8s.io/provider: infrastructure-aws, control-plane-k0sproject-k0smotron, bootstrap-k0sproject-k0smotron\n</code></pre>"},{"location":"template/byo-templates/#compatibility-attributes","title":"Compatibility attributes","text":"<p>Each of the <code>*Template</code> resources has compatibility versions attributes to constraint the core <code>CAPI</code>, <code>CAPI</code> provider or Kubernetes versions. CAPI-related version constraints must be set in the <code>CAPI</code> contract format. Kubernetes version constraints must be set in the Semantic Version format. Each attribute can be set either via the corresponding <code>.spec</code> fields or via the annotations. Values set via the <code>.spec</code> have precedence over the values set via the annotations.</p> <p>Note</p> <p> All of the compatibility attributes are optional, and validation checks only take place if both of the corresponding type attributes (e.g. provider contract versions in both <code>ProviderTemplate</code> and <code>ClusterTemplate</code>) are set.</p> <ol> <li> <p>The <code>ProviderTemplate</code> resource has dedicated fields to set compatible <code>CAPI</code> contract versions along with CRDs contract versions supported by the provider. Given contract versions will be then set accordingly in the <code>.status</code> field. Compatibility contract versions are key-value pairs, where the key is the core <code>CAPI</code> contract version, and the value is an underscore-delimited (_) list of provider contract versions supported by the core <code>CAPI</code>. For the core <code>CAPI</code> Template values should be empty.</p> <p>Example with the <code>.spec</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ProviderTemplate\n# ...\nspec:\n  providers:\n  - infrastructure-aws\n  capiContracts:\n    # commented is the example exclusively for the core CAPI Template\n    # v1alpha3: \"\"\n    # v1alpha4: \"\"\n    # v1beta1: \"\"\n    v1alpha3: v1alpha3\n    v1alpha4: v1alpha4\n    v1beta1: v1beta1_v1beta2\n</code></pre> <p>Example with the <code>annotations</code> in the <code>Chart.yaml</code> with the same logic as in the <code>.spec</code>:</p> <pre><code>annotations:\n  cluster.x-k8s.io/provider: infrastructure-aws\n  cluster.x-k8s.io/v1alpha3: v1alpha3\n  cluster.x-k8s.io/v1alpha4: v1alpha4\n  cluster.x-k8s.io/v1beta1: v1beta1_v1beta2\n</code></pre> </li> <li> <p>The <code>ClusterTemplate</code> resource has dedicated fields to set an exact compatible Kubernetes version in the Semantic Version format and required contract versions per each provider to match against the related <code>ProviderTemplate</code> objects. Given compatibility attributes will be then set accordingly in the <code>.status</code> field. Compatibility contract versions are key-value pairs, where the key is the name of the provider, and the value is the provider contract version required to be supported by the provider.</p> <p>Example with the <code>.spec</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterTemplate\n# ...\nspec:\n  k8sVersion: 1.30.0 # only exact semantic version is applicable\n  providers:\n  - bootstrap-k0sproject-k0smotron\n  - control-plane-k0sproject-k0smotron\n  - infrastructure-aws\n  providerContracts:\n    bootstrap-k0sproject-k0smotron: v1beta1 # only a single contract version is applicable\n    control-plane-k0sproject-k0smotron: v1beta1\n    infrastructure-aws: v1beta2\n</code></pre> <p>Example with the <code>.annotations</code> in the <code>Chart.yaml</code>:</p> <pre><code>annotations:\n  cluster.x-k8s.io/provider: infrastructure-aws, control-plane-k0sproject-k0smotron, bootstrap-k0sproject-k0smotron\n  cluster.x-k8s.io/bootstrap-k0sproject-k0smotron: v1beta1\n  cluster.x-k8s.io/control-plane-k0sproject-k0smotron: v1beta1\n  cluster.x-k8s.io/infrastructure-aws: v1beta2\n  k0rdent.mirantis.com/k8s-version: 1.30.0\n</code></pre> </li> <li> <p>The <code>ServiceTemplate</code> resource has dedicated fields to set an compatibility constrained Kubernetes version to match against the related <code>ClusterTemplate</code> objects. Given compatibility values will be then set accordingly in the <code>.status</code> field.</p> <p>Example with the <code>.spec</code>:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ServiceTemplate\n# ...\nspec:\n  k8sConstraint: \"^1.30.0\" # only semantic version constraints are applicable\n</code></pre> <p>Example with the <code>annotations</code> in the <code>Chart.yaml</code>:</p> <pre><code>k0rdent.mirantis.com/k8s-version-constraint: ^1.30.0\n</code></pre> </li> </ol>"},{"location":"template/byo-templates/#compatibility-attributes-enforcement","title":"Compatibility attributes enforcement","text":"<p>The aforedescribed attributes are being checked sticking to the following rules:</p> <ul> <li>both the exact and constraint version of the same type (e.g. <code>k8sVersion</code> and <code>k8sConstraint</code>) must be set otherwise no check is performed;</li> <li>if a <code>ClusterTemplate</code> object's providers contract version does not satisfy contract versions from the related <code>ProviderTemplate</code> object, the updates to the <code>ClusterDeployment</code> object will be blocked;</li> <li>if a <code>ProviderTemplate</code> object's <code>CAPI</code> contract version (e.g. in a <code>v1beta1: v1beta1_v1beta2</code> key-value pair, the key <code>v1beta1</code> is the core <code>CAPI</code> contract version) is not listed in the core <code>CAPI</code> <code>ProviderTemplate</code> object, the updates to the <code>Management</code> object will be blocked;</li> <li>if a <code>ClusterTemplate</code> object's exact kubernetes version does not satisfy the kubernetes version constraint from the related <code>ServiceTemplate</code> object, the updates to the <code>ClusterDeployment</code> object will be blocked.</li> </ul>"},{"location":"template/byo-templates/#remove-templates-shipped-with-kcm","title":"Remove Templates shipped with kcm","text":"<p>If you need to limit the templates that exist in your kcm installation, follow the instructions below:</p> <ol> <li> <p>Get the list of <code>ProviderTemplates</code>, <code>ClusterTemplates</code> or <code>ServiceTemplates</code> shipped with kcm. For example, for <code>ClusterTemplate</code> objects, run:</p> <pre><code>kubectl get clustertemplates -n kcm-system -l helm.toolkit.fluxcd.io/name=kcm-templates\n</code></pre> <p>Example output:</p> <pre><code>NAME                       VALID\naws-hosted-cp              true\naws-standalone-cp          true\n</code></pre> </li> <li> <p>Remove the template from the list using <code>kubectl delete</code>. For example:</p> <pre><code>kubectl delete clustertemplate -n kcm-system &lt;template-name&gt;\n</code></pre> </li> </ol>"},{"location":"template/main/","title":"Templates system","text":"<p>By default, k0rdent delivers a set of default <code>ProviderTemplate</code>, <code>ClusterTemplate</code> and <code>ServiceTemplate</code> objects:</p> <ul> <li><code>ProviderTemplate</code>    The template containing the configuration of the provider (e.g., k0smotron). Cluster-scoped.</li> <li><code>ClusterTemplate</code>    The template containing the configuration of the cluster objects. Namespace-scoped.</li> <li><code>ServiceTemplate</code>    The template containing the configuration of the service to be installed on the cluster deployment. Namespace-scoped.</li> </ul> <p>All Templates are immutable. You can also build your own templates and use them for deployment along with the templates shipped with k0rdent.</p>"},{"location":"template/main/#template-naming-convention","title":"Template Naming Convention","text":"<p>The templates can have any name. However, since they are immutable, we have adopted a naming convention that includes semver in the name, i.e., <code>template-&lt;major&gt;-&lt;minor&gt;-&lt;patch&gt;</code>. Below are some examples for each of the templates.</p> <p>Example</p> <p>An example of a <code>ProviderTemplate</code> with its status.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ProviderTemplate\nmetadata:\n  name: cluster-api-0-0-4\nspec:\n  helm:\n    chartSpec:\n      chart: cluster-api\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: kcm-templates\n      version: 0.0.4\nstatus:\n  capiContracts:\n    v1alpha3: \"\"\n    v1alpha4: \"\"\n    v1beta1: \"\"\n  chartRef:\n    kind: HelmChart\n    name: cluster-api-0-0-4\n    namespace: kcm-system\n  config:\n    airgap: false\n    config: {}\n    configSecret:\n      create: false\n      name: \"\"\n      namespace: \"\"\n  description: A Helm chart for Cluster API core components\n  observedGeneration: 1\n  valid: true\n</code></pre> <p>Example</p> <p>An example of a <code>ClusterTemplate</code> with its status.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterTemplate\nmetadata:\n  name: aws-standalone-cp-0-0-3\n  namespace: kcm-system\nspec:\n  helm:\n    chartSpec:\n      chart: aws-standalone-cp\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: kcm-templates\n      version: 0.0.3\nstatus:\n  chartRef:\n    kind: HelmChart\n    name: aws-standalone-cp-0-0-3\n    namespace: kcm-system\n  config:\n    bastion:\n      allowedCIDRBlocks: []\n      ami: \"\"\n      disableIngressRules: false\n      enabled: false\n      instanceType: t2.micro\n    clusterIdentity:\n      kind: AWSClusterStaticIdentity\n      name: \"\"\n    clusterNetwork:\n      pods:\n        cidrBlocks:\n        - 10.244.0.0/16\n      services:\n        cidrBlocks:\n        - 10.96.0.0/12\n    controlPlane:\n      amiID: \"\"\n      iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n      imageLookup:\n        baseOS: \"\"\n        format: amzn2-ami-hvm*-gp2\n        org: \"137112412989\"\n      instanceType: \"\"\n      rootVolumeSize: 8\n    controlPlaneNumber: 3\n    extensions:\n      chartRepository: \"\"\n      imageRepository: \"\"\n    k0s:\n      version: v1.31.1+k0s.1\n    publicIP: false\n    region: \"\"\n    sshKeyName: \"\"\n    worker:\n      amiID: \"\"\n      iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n      imageLookup:\n        baseOS: \"\"\n        format: amzn2-ami-hvm*-gp2\n        org: \"137112412989\"\n      instanceType: \"\"\n      rootVolumeSize: 8\n    workersNumber: 2\n  description: 'An kcm template to deploy a k0s cluster on AWS with bootstrapped control\n    plane nodes. '\n  observedGeneration: 1\n  providerContracts:\n    bootstrap-k0sproject-k0smotron: v1beta1\n    control-plane-k0sproject-k0smotron: v1beta1\n    infrastructure-aws: v1beta2\n  providers:\n  - bootstrap-k0sproject-k0smotron\n  - control-plane-k0sproject-k0smotron\n  - infrastructure-aws\n  valid: true\n</code></pre> <p>Example</p> <p>An example of a <code>ServiceTemplate</code> with its status.</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ServiceTemplate\nmetadata:\n  name: kyverno-3-2-6\n  namespace: kcm-system\nspec:\n  helm:\n    chartSpec:\n      chart: kyverno\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: kcm-templates\n      version: 3.2.6\nstatus:\n  chartRef:\n    kind: HelmChart\n    name: kyverno-3-2-6\n    namespace: kcm-system\n  description: A Helm chart to refer the official kyverno helm chart\n  observedGeneration: 1\n  valid: true\n</code></pre>"},{"location":"template/main/#template-life-cycle-management","title":"Template Life Cycle Management","text":"<p>Cluster and Service Templates can be delivered to target namespaces using the <code>AccessManagement</code>, <code>ClusterTemplateChain</code> and <code>ServiceTemplateChain</code> objects. <code>AccessManagement</code> object contains the list of access rules to apply. Each access rule contains the namespaces' definition to deliver templates into and the template chains. Each <code>ClusterTemplateChain</code> and <code>ServiceTemplateChain</code> contains the supported templates and the upgrade sequences for them.</p> <p>The example of the Cluster Template Management:</p> <ol> <li>Create <code>ClusterTemplateChain</code> object in the system namespace (defaults to <code>kcm-system</code>). Properly configure    the list of <code>.spec.supportedTemplates[].availableUpgrades</code> for the specified <code>ClusterTemplate</code> if the upgrade is allowed. For example:</li> </ol> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterTemplateChain\nmetadata:\n  name: aws\n  namespace: kcm-system\nspec:\n  supportedTemplates:\n    - name: aws-standalone-cp-0-0-1\n      availableUpgrades:\n        - name: aws-standalone-cp-0-0-2\n    - name: aws-standalone-cp-0-0-2\n</code></pre> <ol> <li>Edit <code>AccessManagement</code> object and configure the <code>.spec.accessRules</code>.    For example, to apply all templates and upgrade sequences defined in the <code>aws</code> <code>ClusterTemplateChain</code> to the    <code>default</code> namespace, the following <code>accessRule</code> should be added:</li> </ol> <pre><code>spec:\n  accessRules:\n  - targetNamespaces:\n      list:\n        - default\n    clusterTemplateChains:\n      - aws\n</code></pre> <p>The kcm controllers will deliver all the <code>ClusterTemplate</code> objects across the target namespaces. As a result, the new objects should be created:</p> <ul> <li><code>ClusterTemplateChain</code> <code>default/aws</code></li> <li><code>ClusterTemplate</code> <code>default/aws-standalone-cp-0-0-1</code></li> <li><code>ClusterTemplate</code> <code>default/aws-standalone-cp-0-0-2</code> (available for the upgrade from <code>aws-standalone-cp-0-0-1</code>)</li> </ul> <p>Note</p> <ol> <li>The target <code>ClusterTemplate</code> defined as the available for the upgrade should reference the same helm chart name as the source <code>ClusterTemplate</code>. Otherwise, after the upgrade is triggered, the cluster will be removed and then, recreated from scratch even if the objects in the helm chart are the same.</li> <li>The target template should not affect immutable fields or any other incompatible internal objects upgrades, otherwise the upgrade will fail.</li> </ol>"},{"location":"template/pre-defined-service-templates/","title":"Pre-defined ServiceTemplates","text":"<p>kcm comes with a set of pre-defined ServiceTemplates. These templates are referring to the following Helm charts:</p> <p>Note</p> <p> Listed Helm charts will be looked up in the registry configured by the <code>--default-registry-url</code> flag provided to the <code>kcm-controller-manager</code> (default value: <code>oci://ghcr.io/k0rdent/kcm/charts</code>). In case of using different registry, the corresponding Helm charts must be available in the registry in prior to using the ServiceTemplate.</p> Application Chart version Source cert-manager 1.16.2 https://charts.jetstack.io external-secrets 0.11.0 https://charts.external-secrets.io dex 0.19.1 https://charts.dexidp.io velero 8.1.0 https://vmware-tanzu.github.io/helm-charts/ ingress-nginx 4.11.0 https://kubernetes.github.io/ingress-nginx ingress-nginx 4.11.3 https://kubernetes.github.io/ingress-nginx kyverno 3.2.6 https://kyverno.github.io/kyverno/"},{"location":"template/pre-defined-service-templates/#configuration-requirements","title":"Configuration Requirements","text":"<p>The pre-defined ServiceTemplates are provided without any pre-configured values. Several of the listed charts require additional configuration before they can be effectively used:</p> <ul> <li>dex: Requires issuer URL, storage type and at least one connector to be configured</li> <li>velero: Requires provider's credentials to be configured</li> <li>cert-manager: Requires CRDs to be installed</li> </ul> <p>Before using these ServiceTemplates, make sure to review each chart's documentation and configure the necessary values according to your environment requirements.</p>"},{"location":"usage/adopt-existing-cluster/","title":"Adopting existing Kubernetes cluster guide","text":""},{"location":"usage/adopt-existing-cluster/#prerequisites","title":"Prerequisites","text":"<p>In order to adopt an existing Kubernetes cluster you will require the following:</p> <ul> <li>A kubernetes kubeconfig file for the cluster to be adopted</li> <li>A management cluster with kcm installed</li> <li>Network connectivity between the management cluster and the cluster to be adopted</li> </ul>"},{"location":"usage/adopt-existing-cluster/#installation","title":"Installation","text":""},{"location":"usage/adopt-existing-cluster/#step-1-create-the-credential","title":"Step 1: Create the credential","text":"<p>Create a <code>Credential</code> object with all credentials required per the   Credential System.</p>"},{"location":"usage/adopt-existing-cluster/#step-2-configure-the-adopted-cluster-template","title":"Step 2: Configure the adopted cluster template","text":"<ul> <li>Set the <code>KUBECONFIG</code> environment variable to the path to the management   cluster kubeconfig file.</li> </ul>"},{"location":"usage/adopt-existing-cluster/#step-3-create-the-clusterdeployment-object-yaml-configuration","title":"Step 3: Create the ClusterDeployment Object YAML Configuration","text":"<ul> <li> <p>Create the file with the <code>ClusterDeployment</code> configuration:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;cluster-name&gt;\n  namespace: &lt;kcm-system-namespace&gt;\nspec:\n  template: adopted-cluster-&lt;template-version&gt;\n  credential: &lt;credential-name&gt;\n  dryRun: &lt;\"true\" or \"false\": defaults to \"false\"&gt;\n  config:\n    &lt;cluster-configuration&gt;\n</code></pre> </li> </ul> <p>Note</p> <p> Substitute the parameters enclosed in angle brackets with the corresponding values. Enable the <code>dryRun</code> flag if required. For details, see Dry Run.</p> <p>Following is an interpolated example.</p> <p>Example</p> <p><code>ClusterDeployment</code> </p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-cluster\n  namespace: kcm-system\nspec:\n  template: adotped-cluster-0-0-2\n  credential: my-cluster-credential\n  dryRun: true\n  config: {}\n</code></pre>"},{"location":"usage/adopt-existing-cluster/#step-4-apply-the-clusterdeployment-configuration-to-create-it","title":"Step 4: Apply the <code>ClusterDeployment</code> Configuration to Create it","text":"<ul> <li>Apply the <code>ClusterDeployment</code> object to your k0rdent deployment:</li> </ul> <pre><code>kubectl apply -f clusterdeployment.yaml\n</code></pre>"},{"location":"usage/adopt-existing-cluster/#step-5-check-the-status-of-the-clusterdeployment-object","title":"Step 5: Check the Status of the <code>ClusterDeployment</code> Object","text":"<ul> <li>Check the status of the newly created <code>ClusterDeployment</code>:</li> </ul> <pre><code>kubectl -n &lt;namespace&gt; get clusterdeployment.kcm &lt;cluster-name&gt; -o=yaml\n</code></pre>"},{"location":"usage/airgap/","title":"Air-gapped Installation Guide","text":"<p>Warning</p> <p> Currently only vSphere infrastructure provider supports full air-gapped installation.</p>"},{"location":"usage/airgap/#prerequisites","title":"Prerequisites","text":"<p>In order to install kcm in an air-gapped environment, you need will need the following:</p> <ul> <li>An installed k0s cluster that will be used as the management cluster.  If you   do not yet have a k0s cluster, you can follow the Airgapped Installation   documentation.  k0s is recommended for airgapped installations because it   implements an OCI image bundle watcher which allows k0s to utilize a bundle   of management cluster images easily. Any Kubernetes distribution can be   used, but instructions for using k0s are provided here.</li> <li>The <code>KUBECONFIG</code> of a management cluster that will be the target for the kcm   installation.</li> <li> <p>A registry that is accessible from the airgapped hosts to store the kcm images.   If you do not have a registry you can deploy a local Docker registry   or use mindthegap</p> <p>Warning</p> <p> If using a local Docker registry, ensure the registry URL is added to the <code>insecure-registries</code> key within the Docker <code>/etc/docker/daemon.json</code> file. <pre><code>{\n  \"insecure-registries\": [\"&lt;registry-url&gt;\"]\n}\n</code></pre></p> </li> <li> <p>A registry and associated chart repository for hosting kcm charts.  At this   time all kcm charts MUST be hosted in a single OCI chart repository.  See   Use OCI-based registries in the   Helm documentation for more information.</p> </li> <li>jq, Helm and Docker binaries   installed on the machine where the <code>airgap-push.sh</code> script will be run.</li> </ul>"},{"location":"usage/airgap/#installation","title":"Installation","text":"<ol> <li> <p>Download the kcm airgap bundle, the bundle contains the following:</p> <ul> <li><code>images/kcm-images-&lt;version&gt;.tgz</code> - The image bundle tarball for the   management cluster, this bundle will be loaded into the management   cluster.</li> <li><code>images/kcm-extension-images-&lt;version&gt;.tgz</code> - The image bundle tarball for   the managed clusters, this bundle will be pushed to a registry where the   images can be accessed by the managed clusters.</li> <li><code>charts</code> - Contains the kcm Helm chart, dependency charts and k0s   extensions charts within the <code>extensions</code> directory.  All of these charts   will be pushed to a chart repository within a registry.</li> <li><code>scripts/airgap-push.sh</code> - A script that will aid in re-tagging and   pushing the <code>ClusterDeployment</code> required charts and images to a desired   registry.</li> </ul> </li> <li> <p>Extract and use the <code>airgap-push.sh</code> script to push the <code>extensions</code> images    and <code>charts</code> contents to the registry.  Ensure you have logged into the    registry using both <code>docker login</code> and <code>helm registry login</code> before running    the script.</p> <pre><code>tar xvf kcm-airgap-&lt;version&gt;.tgz scripts/airgap-push.sh\n./scripts/airgap-push.sh -r &lt;registry&gt; -c &lt;chart-repo&gt; -a kcm-airgap-&lt;version&gt;.tgz\n</code></pre> </li> <li> <p>Next, extract the <code>management</code> bundle tarball and sync the images to the    k0s cluster which will host the management cluster.  See Sync the Bundle File    for more information.</p> <p>Note</p> <p> Multiple image bundles can be placed in the <code>/var/lib/k0s/images</code> directory for k0s to use and the existing <code>k0s</code> airgap bundle does not need to be merged into the <code>kcm-images-&lt;version&gt;.tgz</code> bundle.</p> <pre><code>tar -C /var/lib/k0s -xvf kcm-airgap-&lt;version&gt;.tgz \"images/kcm-images-&lt;version&gt;.tgz\"\n</code></pre> </li> <li> <p>Install the kcm Helm chart on the management cluster from the registry where    the kcm charts were pushed.  The kcm controller image is loaded as part of    the airgap <code>management</code> bundle and does not need to be customized within the    Helm chart, but the default chart repository configured via    <code>controller.defaultRegistryURL</code> should be set to reference the repository    where charts have been pushed.</p> <pre><code>helm install kcm oci://&lt;chart-repository&gt;/kcm \\\n  --version &lt;version&gt; \\\n  -n kcm-system \\\n  --create-namespace \\\n  --set controller.defaultRegistryURL=oci://&lt;chart-repository&gt;\n</code></pre> </li> <li> <p>Edit the <code>Management</code> object to add the airgap parameters.</p> <p>Note</p> <p> Use <code>insecureRegistry</code> parameter only in case if you have plain HTTP registry.</p> <p>The resulting yaml may look like this:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  core:\n    capi:\n      config:\n        airgap: true\n    kcm:\n      config:\n        controller:\n          defaultRegistryURL: oci://&lt;registry-url&gt;\n          insecureRegistry: true\n  providers:\n  - config:\n      airgap: true\n    name: k0smotron\n  - config:\n      airgap: true\n    name: cluster-api-provider-vsphere\n  - name: projectsveltos\n  release: &lt;release name&gt;\n</code></pre> </li> <li> <p>Place k0s binary and airgap bundle at internal server, so they could be    available over HTTP. This is required for the airgap provisioning process,    since k0s components must be downloaded at each node upon creation.    Alternatively you can create the following example deployment using the k0s    image provided in the bundle.</p> <p>Note</p> <p> k0s image version is the same that the default defined in the vSphere template.</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k0s-ag-image\n  labels:\n    app: k0s-ag-image\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k0s-ag-image\n  template:\n    metadata:\n      labels:\n        app: k0s-ag-image\n    spec:\n      containers:\n      - name: k0s-ag-image\n        image: k0s-ag-image:v1.31.1-k0s.1\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: k0s-ag-image\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: k0s-ag-image\n  type: NodePort\n</code></pre> </li> </ol>"},{"location":"usage/airgap/#creation-of-the-clusterdeployment","title":"Creation of the ClusterDeployment","text":"<p>In order to successfully deploy a cluster several configuration options must be defined in the <code>.spec.config</code> of the `ClusterDeployment.</p> <p>You must specify the custom image registry and chart repository to be used (the registry and chart repository where the <code>extensions</code> bundle and charts were pushed).</p> <p>Apart from that you must provide endpoint where k0s binary and airgap bundle could be downloaded (step <code>6</code> of the installation procedure)</p> <pre><code>spec:\n config:\n   airgap: true\n   k0s:\n     downloadURL: \"http://&lt;k0s binary endpoint&gt;/k0s\"\n     bundleURL: \"http://&lt;k0s binary endpoint&gt;/k0s-airgap-bundle\"\n   extensions:\n    imageRepository: ${IMAGE_REPOSITORY}\n    chartRepository: ${CHART_REPOSITORY}\n</code></pre>"},{"location":"usage/cluster-update/","title":"Cluster Deployment update","text":"<p>To update the <code>ClusterDeployment</code>, update <code>.spec.template</code> in the <code>ClusterDeployment</code> object to the new <code>ClusterTemplate</code> name:</p> <p>Run:</p> <pre><code>kubectl patch clusterdeployment.kcm &lt;cluster-name&gt; -n &lt;namespace&gt; --patch '{\"spec\":{\"template\":\"&lt;new-template-name&gt;\"}}' --type=merge\n</code></pre> <p>Then, check the status of the <code>ClusterDeployment</code> object:</p> <pre><code>kubectl get clusterdeployment.kcm &lt;cluster-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>In the commands above, replace the parameters enclosed in angle brackets with the corresponding values.</p> <p>To get more details, run the previous command with the <code>-o=yaml</code> option and check the <code>.status.conditions</code>.</p> <p>Note</p> <p> The <code>ClusterDeployment</code> is allowed to be updated to specific templates only. The templates available for the update are defined in the <code>ClusterTemplateChain</code> objects. Also, the <code>AccessManagement</code> object should contain properly configured <code>spec.accessRules</code> with the list of <code>ClusterTemplateChain</code> object names and the namespaces where the supported templates from the chain spec will be delivered. For details, see: Template Life Cycle Management.</p>"},{"location":"usage/create-cluster-deployment/","title":"Create Cluster Deployment","text":""},{"location":"usage/create-cluster-deployment/#creation-process","title":"Creation Process","text":""},{"location":"usage/create-cluster-deployment/#step-1-create-credential","title":"Step 1: Create Credential","text":"<ul> <li>Create a <code>Credential</code> object with all credentials required per the   Credential System.</li> </ul>"},{"location":"usage/create-cluster-deployment/#step-2-select-the-template","title":"Step 2: Select the Template","text":"<p>For details about the templates in k0rdent, see the Templates system.</p> <ul> <li>Set the <code>KUBECONFIG</code> environment variable to the path to the management   cluster kubeconfig file. Then select the <code>Template</code> you want to use for the   deployment. To list all available templates, run:</li> </ul> <pre><code>kubectl get clustertemplate -n kcm-system\n</code></pre> <p>Note</p> <p> If you want to deploy a hosted control plane template, check additional notes on hosted control planes for each of the clustertemplate sections:</p> <ul> <li>AWS Hosted Control Plane</li> <li>vSphere Hosted Control Plane</li> </ul>"},{"location":"usage/create-cluster-deployment/#step-3-create-the-clusterdeployment-object-yaml-configuration","title":"Step 3: Create the ClusterDeployment Object YAML Configuration","text":"<ul> <li> <p>Create the file with the <code>ClusterDeployment</code> configuration:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: &lt;cluster-name&gt;\n  namespace: &lt;kcm-system-namespace&gt;\nspec:\n  template: &lt;template-name&gt;\n  credential: &lt;infrastructure-provider-credential-name&gt;\n  dryRun: &lt;\"true\" or \"false\": defaults to \"false\"&gt;\n  config:\n    &lt;cluster-configuration&gt;\n</code></pre> </li> </ul> <p>Note</p> <p> Substitute the parameters enclosed in angle brackets with the corresponding values. Enable the <code>dryRun</code> flag if required. For details, see Dry Run.</p> <p>Following is an interpolated example.</p> <p>Example</p> <p><code>ClusterDeployment</code> for AWS Infrastructure Provider Object Example</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-managed-cluster\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-3\n  credential: aws-credential\n  dryRun: true\n  config:\n    region: us-west-2\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n</code></pre>"},{"location":"usage/create-cluster-deployment/#step-4-apply-the-clusterdeployment-configuration-to-create-it","title":"Step 4: Apply the <code>ClusterDeployment</code> Configuration to Create it","text":"<ul> <li> <p>Apply the <code>ClusterDeployment</code> object to your k0rdent deployment:</p> <pre><code>kubectl apply -f clusterdeployment.yaml\n</code></pre> </li> </ul>"},{"location":"usage/create-cluster-deployment/#step-5-check-the-status-of-the-clusterdeployment-object","title":"Step 5: Check the Status of the <code>ClusterDeployment</code> Object","text":"<ul> <li> <p>Check the status of the newly created <code>ClusterDeployment</code>:</p> <pre><code>kubectl -n &lt;namespace&gt; get clusterdeployment.kcm &lt;cluster-name&gt; -o=yaml\n</code></pre> </li> </ul> <p>Info</p> <p> Reminder: <code>&lt;namespace&gt;</code> and <code>&lt;cluster-name&gt;</code> are defined in the <code>.metadata</code> section of the <code>ClusterDeployment</code> object you created above.</p>"},{"location":"usage/create-cluster-deployment/#step-6-wait-for-infrastructure-and-cluster-to-be-provisioned","title":"Step 6: Wait for Infrastructure and Cluster to be Provisioned","text":"<ul> <li> <p>Wait for infrastructure to be provisioned and the cluster to be deployed:</p> <pre><code>kubectl -n &lt;namespace&gt; get cluster &lt;cluster-name&gt; -o=yaml\n</code></pre> </li> </ul> <p>Tip</p> <p> You may also watch the process with the <code>clusterctl describe</code> command (requires the <code>clusterctl</code> CLI to be installed):</p> <pre><code>clusterctl describe cluster &lt;cluster-name&gt; -n &lt;namespace&gt; --show-conditions all\n</code></pre>"},{"location":"usage/create-cluster-deployment/#step-7-retrieve-kubernetes-configuration-of-your-cluster-deployment","title":"Step 7: Retrieve Kubernetes Configuration of Your Cluster Deployment","text":"<ul> <li> <p>Retrieve the Kubernetes configuration of your cluster deployment when it is   finished provisioning:</p> <pre><code>kubectl get secret -n &lt;namespace&gt; &lt;cluster-name&gt;-kubeconfig -o=jsonpath={.data.value} | base64 -d &gt; kubeconfig\n</code></pre> </li> </ul>"},{"location":"usage/create-cluster-deployment/#dry-run","title":"Dry Run","text":"<p>k0rdent <code>ClusterDeployment</code> supports two modes: with and without <code>.spec.dryRun</code> (defaults to <code>false</code>).</p> <p>If no configuration (<code>.spec.config</code>) is specified, the <code>ClusterDeployment</code> object will be populated with defaults (default configuration can be found in the corresponding <code>Template</code> status) and automatically have <code>.spec.dryRun</code> set to <code>true</code>.</p> <p>Example</p> <p><code>ClusterDeployment</code> with default configuration</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-managed-cluster\n  namespace: kcm-system\nspec:\n  config:\n    clusterNetwork:\n      pods:\n        cidrBlocks:\n        - 10.244.0.0/16\n      services:\n        cidrBlocks:\n        - 10.96.0.0/12\n    controlPlane:\n      iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n      instanceType: \"\"\n    controlPlaneNumber: 3\n    k0s:\n      version: v1.27.2+k0s.0\n    publicIP: false\n    region: \"\"\n    sshKeyName: \"\"\n    worker:\n      amiID: \"\"\n      iamInstanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io\n      instanceType: \"\"\n    workersNumber: 2\n  template: aws-standalone-cp-0-0-3\n  credential: aws-credential\n  dryRun: true\n</code></pre> <p>After you adjust your configuration and ensure that it passes validation (<code>TemplateReady</code> condition from <code>.status.conditions</code>), remove the <code>.spec.dryRun</code> flag to proceed with the deployment.</p> <p>Here is an example of a <code>ClusterDeployment</code> object that passed the validation:</p> <p>Example</p> <p><code>ClusterDeployment</code> object that passed the validation</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-managed-cluster\n  namespace: kcm-system\nspec:\n  template: aws-standalone-cp-0-0-3\n  credential: aws-credential\n  config:\n    region: us-east-2\n    publicIP: true\n    controlPlaneNumber: 1\n    workersNumber: 1\n    controlPlane:\n      instanceType: t3.small\n    worker:\n      instanceType: t3.small\n  status:\n    conditions:\n    - lastTransitionTime: \"2024-07-22T09:25:49Z\"\n      message: Template is valid\n      reason: Succeeded\n      status: \"True\"\n      type: TemplateReady\n    - lastTransitionTime: \"2024-07-22T09:25:49Z\"\n      message: Helm chart is valid\n      reason: Succeeded\n      status: \"True\"\n      type: HelmChartReady\n    - lastTransitionTime: \"2024-07-22T09:25:49Z\"\n      message: ClusterDeployment is ready\n      reason: Succeeded\n      status: \"True\"\n      type: Ready\n    observedGeneration: 1\n</code></pre>"},{"location":"usage/create-cluster-deployment/#cleanup","title":"Cleanup","text":"<ol> <li> <p>Remove the Management object:</p> <pre><code>kubectl delete management.kcm kcm\n</code></pre> </li> </ol> <p>Note</p> <p> Ensure you have no k0rdent <code>ClusterDeployment</code> objects left in the cluster prior to Management deletion.</p> <ol> <li> <p>Remove the <code>kcm</code> Helm release:</p> <pre><code>helm uninstall kcm -n kcm-system\n</code></pre> </li> <li> <p>Remove the <code>kcm-system</code> namespace:</p> <pre><code>kubectl delete ns kcm-system\n</code></pre> </li> </ol>"},{"location":"usage/create-multiclusterservice/","title":"Deploy beach-head services using MultiClusterService","text":"<p>The <code>MultiClusterService</code> object is used to deploy beach-head services on multiple matching clusters.</p>"},{"location":"usage/create-multiclusterservice/#creation","title":"Creation","text":"<p>The <code>MultiClusterService</code> object can be created with the following YAML:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: MultiClusterService\nmetadata:\n  name: &lt;name&gt;\nspec:\n  clusterSelector:\n    matchLabels:\n      &lt;key1&gt;: &lt;value1&gt;\n      &lt;key2&gt;: &lt;value2&gt;\n      . . .\n  services:\n  - template: &lt;servicetemplate-1-name&gt;\n    name: &lt;release-name&gt;\n    namespace: &lt;release-namespace&gt;\n  servicesPriority: 100\n  stopOnConflict: false\n</code></pre>"},{"location":"usage/create-multiclusterservice/#matching-multiple-clusters","title":"Matching Multiple Clusters","text":"<p>Consider the following example where 2 clusters have been deployed using ClusterDeployment objects: <pre><code>\u279c  ~ kubectl get clusterdeployments.k0rdent.mirantis.com -n kcm-system\nNAME             READY   STATUS\ndev-cluster-1   True    ClusterDeployment is ready\ndev-cluster-2   True    ClusterDeployment is ready\n\u279c  ~ \n\u279c  ~ \n\u279c  ~  kubectl get cluster -n kcm-system --show-labels\nNAME           CLUSTERCLASS     PHASE         AGE     VERSION   LABELS\ndev-cluster-1                  Provisioned   2h41m             app.kubernetes.io/managed-by=Helm,helm.toolkit.fluxcd.io/name=dev-cluster-1,helm.toolkit.fluxcd.io/namespace=kcm-system,sveltos-agent=present\ndev-cluster-2                  Provisioned   3h10m             app.kubernetes.io/managed-by=Helm,helm.toolkit.fluxcd.io/name=dev-cluster-2,helm.toolkit.fluxcd.io/namespace=kcm-system,sveltos-agent=present\n</code></pre></p> <p>Example</p> <p>Spec for <code>dev-cluster-1</code> ClusterDeployment (only sections relevant to beach-head services): <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  . . . \n  name: dev-cluster-1\n  namespace: kcm-system\nspec:\n  . . .\n  services:\n  - name: kyverno\n    namespace: kyverno\n    template: kyverno-3-2-6\n  - name: ingress-nginx\n    namespace: ingress-nginx\n    template: ingress-nginx-4-11-0\n  servicesPriority: 100\n  stopOnConflict: false\n  . . .\n</code></pre></p> <p>Spec for <code>dev-cluster-2</code> ClusterDeployment (only sections relevant to beach-head services): <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  . . .\n  name: dev-cluster-2\n  namespace: kcm-system\nspec:\n  . . .\n  services:\n  - name: ingress-nginx\n    namespace: ingress-nginx\n    template: ingress-nginx-4-11-0\n  servicesPriority: 500\n  stopOnConflict: false\n  . . .\n</code></pre></p> <p>Note</p> <p>See Deploy beach-head Services using Cluster Deployment for how to use beach-head services with ClusterDeployment.</p> <p>Now the following <code>global-ingress</code> MultiClusterService object is created with the following spec:</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: MultiClusterService\nmetadata:\n  name: global-ingress\nspec:\n  clusterSelector:\n    matchLabels:\n      app.kubernetes.io/managed-by: Helm\n  services:\n  - name: ingress-nginx\n    namespace: ingress-nginx\n    template: ingress-nginx-4-11-3\n  servicesPriority: 300\n  stopOnConflict: false\n</code></pre> <p>This MultiClusterService will match any CAPI cluster with the label <code>app.kubernetes.io/managed-by: Helm</code> and deploy version 4.11.3 of ingress-nginx service on it.</p>"},{"location":"usage/create-multiclusterservice/#configuring-custom-values","title":"Configuring Custom Values","text":"<p>Refer to \"Configuring Custom Values\" in Deploy beach-head Services using Cluster Deployment.</p>"},{"location":"usage/create-multiclusterservice/#templating-custom-values","title":"Templating Custom Values","text":"<p>Refer to \"Templating Custom Values\" in Deploy beach-head Services using Cluster Deployment.</p>"},{"location":"usage/create-multiclusterservice/#services-priority-and-conflict","title":"Services Priority and Conflict","text":"<p>The <code>.spec.servicesPriority</code> field is used to specify the priority for the services managed by a ClusterDeployment or MultiClusterService object. Considering the example above:</p> <ol> <li>ClusterDeployment <code>dev-cluster-1</code> manages deployment of kyverno (v3.2.6) and ingress-nginx (v4.11.0) with <code>servicesPriority=100</code> on its cluster.</li> <li>ClusterDeployment <code>dev-cluster-2</code> manages deployment of ingress-nginx (v4.11.0) with <code>servicesPriority=500</code> on its cluster.</li> <li>MultiClusterService <code>global-ingress</code> manages deployment of ingress-nginx (v4.11.3) with <code>servicesPriority=300</code> on both clusters.</li> </ol> <p>This scenario presents a conflict on both the clusters as the MultiClusterService is attempting to deploy v4.11.3 of ingress-nginx on both whereas the ClusterDeployment for each is attempting to deploy v4.11.0 of ingress-nginx.</p> <p>This is where <code>.spec.servicesPriority</code> can be used to specify who gets the priority. Higher number means higer priority and vice versa. In this example: 1. MultiClusterService \"global-ingress\" will take precedence over ClusterDeployment \"dev-cluster-1\" and ingress-nginx (v4.11.3) defined in MultiClusterService object will be deployed on the cluster. 2. ClusterDeployment \"dev-cluster-2\" will take precedence over MultiClusterService \"global-ingress\" and ingress-nginx (v4.11.0) defined in ClusterDeployment object will be deployed on the cluster.</p> <p>Note</p> <p>If servicesPriority are equal, the first one to reach the cluster wins and deploys its beach-head services.</p>"},{"location":"usage/create-multiclusterservice/#checking-status","title":"Checking Status","text":"<p>The status for the MultiClusterService object will show the deployment status for the beach-head services managed by it on each of the CAPI target clusters that it matches. Consider the same example where 2 ClusterDeployments and 1 MultiClusterService is deployed.</p> <p>Example</p> <p>Status for <code>global-ingress</code> MultiClusterService <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: MultiClusterService\nmetadata:\n  . . .\n  name: global-ingress\n  resourceVersion: \"38146\"\n  . . .\nspec:\n  clusterSelector:\n    matchLabels:\n      app.kubernetes.io/managed-by: Helm\n  services:\n  - name: ingress-nginx\n    namespace: ingress-nginx\n    template: ingress-nginx-4-11-3\n  servicesPriority: 300\n  stopOnConflict: false\nstatus:\n  conditions:\n  - lastTransitionTime: \"2024-10-25T08:36:24Z\"\n    message: \"\"\n    reason: Succeeded\n    status: \"True\"\n    type: SveltosClusterProfileReady\n  - lastTransitionTime: \"2024-10-25T08:36:24Z\"\n    message: MultiClusterService is ready\n    reason: Succeeded\n    status: \"True\"\n    type: Ready\n  observedGeneration: 1\n  services:\n  - clusterName: dev-cluster-2\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:36:35Z\"\n      message: |\n        cannot manage chart ingress-nginx/ingress-nginx. ClusterSummary p--dev-cluster-2-capi-dev-cluster-2 managing it.\n      reason: Failed\n      status: \"False\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T08:36:25Z\"\n      message: 'Release ingress-nginx/ingress-nginx: ClusterSummary p--dev-cluster-2-capi-dev-cluster-2\n        managing it'\n      reason: Conflict\n      status: \"False\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n  - clusterName: dev-cluster-1\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:36:24Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T08:36:25Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre></p> <p>The status under <code>.status.services</code> shows a conflict for <code>dev-cluster-2</code> as expected because the MultiClusterService has a lower priority. Whereas, it shows provisioned for <code>dev-cluster-1</code> because the MultiClusterService has a higher priority.</p> <p>Example</p> <p>Status for <code>dev-cluster-1</code> ClusterDeployment (only sections relevant to beach-head services): <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  . . . \n  name: dev-cluster-1\n  namespace: kcm-system\n  . . .\nspec:\n  . . .\n  services:\n  - name: kyverno\n    namespace: kyverno\n    template: kyverno-3-2-6\n  - name: ingress-nginx\n    namespace: ingress-nginx\n    template: ingress-nginx-4-11-0\n  servicesPriority: 100\n  stopOnConflict: false\n  . . .\nstatus:\n  . . .\n  services:\n  - clusterName: dev-cluster-1\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:36:35Z\"\n      message: |\n        cannot manage chart ingress-nginx/ingress-nginx. ClusterSummary global-ingress-capi-dev-cluster-1 managing it.\n      reason: Provisioning\n      status: \"False\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T07:44:43Z\"\n      message: Release kyverno/kyverno\n      reason: Managing\n      status: \"True\"\n      type: kyverno.kyverno/SveltosHelmReleaseReady\n    - lastTransitionTime: \"2024-10-25T08:36:25Z\"\n      message: 'Release ingress-nginx/ingress-nginx: ClusterSummary global-ingress-capi-dev-cluster-1\n        managing it'\n      reason: Conflict\n      status: \"False\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre></p> <p>The status under <code>.status.services</code> for ClusterDeployment <code>dev-cluster-1</code> shows that it is managing kyverno but unable to manage ingress-nginx because another object with higher priority is managing it, so it shows a conflict instead.</p> <p>Example</p> <p>Status for <code>dev-cluster-2</code> ClusterDeployment (only sections relevant to beach-head services): <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  . . .\n  name: dev-cluster-2\n  namespace: kcm-system\n  resourceVersion: \"30889\"\n  . . .\nspec:\n  . . .\n  services:\n  - name: ingress-nginx\n    namespace: ingress-nginx\n    template: ingress-nginx-4-11-0\n  servicesPriority: 500\n  stopOnConflict: false\n  . . .\nstatus:\n  . . .\n  services:\n  - clusterName: dev-cluster-2\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-10-25T08:18:22Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-10-25T08:18:22Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre></p> <p>The status under <code>.status.services</code> for ClusterDeployment <code>dev-cluster-2</code> shows that it is managing ingress-nginx as expected since it has a higher priority.</p>"},{"location":"usage/create-multiclusterservice/#parameter-list","title":"Parameter List","text":"<p>Refer to \"Parameter List\" in Deploy beach-head Services using Cluster Deployment.</p>"},{"location":"usage/deploy-services-clusterdeployment/","title":"Deploy beach-head Services using Cluster Deployment","text":""},{"location":"usage/deploy-services-clusterdeployment/#deployment","title":"Deployment","text":"<p>Beach-head services can be installed on a cluster deployment (i.e., target cluster) using the <code>ClusterDeployment</code> object. Consider the following example:</p> <p>Example</p> <p><code>ClusterDeployment</code> object for AWS Infrastructure Provider with beach-head services</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-managed-cluster\n  namespace: kcm-system\nspec:\n  config:\n    clusterNetwork:\n      pods:\n        cidrBlocks:\n        - 10.244.0.0/16\n      services:\n        cidrBlocks:\n        - 10.96.0.0/12\n    controlPlane:\n      iamInstanceProfile: control-plane.cluster-api-provider-aws.sigs.k8s.io\n      instanceType: \"\"\n    controlPlaneNumber: 3\n    k0s:\n      version: v1.27.2+k0s.0\n    publicIP: false\n    region: \"\"\n    sshKeyName: \"\"\n    worker:\n      amiID: \"\"\n      iamInstanceProfile: nodes.cluster-api-provider-aws.sigs.k8s.io\n      instanceType: \"\"\n    workersNumber: 2\n  template: aws-standalone-cp-0-0-3\n  credential: aws-credential\n  services:\n    - template: kyverno-3-2-6\n      name: kyverno\n      namespace: kyverno\n    - template: ingress-nginx-4-11-3\n      name: ingress-nginx\n      namespace: ingress-nginx\n  servicesPriority: 100\n  stopOnConflict: false\n</code></pre> <p>In the example above the following fields are relevant to the deployment of beach-head services:</p> <pre><code>  . . .\n  services:\n    - template: kyverno-3-2-6\n      name: kyverno\n      namespace: kyverno\n    - template: ingress-nginx-4-11-3\n      name: ingress-nginx\n      namespace: ingress-nginx\n  servicesPriority: 100\n  stopOnConflict: false\n  template: aws-standalone-cp-0-0-3\n</code></pre> <p>Note</p> <p> Refer to Parameter List for more detail about these fields.</p> <p>This example <code>ClusterDeployment</code> object will deploy kyverno and ingress-nginx referred to by their service templates respectively on the target cluster it is managing. See the example below for the service template for kyverno.</p> <p>Example</p> <p><code>ServiceTemplate</code> object for kyverno version 3.2.6 The <code>ServiceTemplate</code> for kyverno: <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ServiceTemplate\nmetadata:\n  name: kyverno-3-2-6\n  namespace: kcm-system\nspec:\n  helm:\n    chartSpec:\n      chart: kyverno\n      interval: 10m0s\n      reconcileStrategy: ChartVersion\n      sourceRef:\n        kind: HelmRepository\n        name: kcm-templates\n      version: 3.2.6\n</code></pre></p> <p>The <code>kcm-templates</code> helm repository hosts the actual chart for kyverno version 3.2.6. For more details see the Bring your own Templates guide.</p>"},{"location":"usage/deploy-services-clusterdeployment/#configuring-custom-values","title":"Configuring Custom Values","text":"<p>Helm values can be passed to each beach-head services with the <code>.spec.services[].values</code> field in the <code>ClusterDeployment</code> or <code>MultiClusterService</code> object.</p> <p>Example</p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  . . .\n  name: my-clusterdeployment\n  namespace: kcm-system\n  . . .\nspec:\n  . . .\n  services:\n   . . .\n    - name: cert-manager\n      namespace: cert-manager\n      template: cert-manager-1-16-1\n      values: |\n        crds:\n          enabled: true\n    - name: motel-regional\n      namespace: motel\n      template: motel-regional-0-1-1\n      values: |\n        victoriametrics:\n          vmauth:\n            ingress:\n              host: vmauth.kcm0.example.net\n            credentials:\n              username: motel\n              password: motel\n        grafana:\n          ingress:\n            host: grafana.kcm0.example.net\n        cert-manager:\n          email: mail@example.net\n    - template: ingress-nginx-4-11-3\n      name: ingress-nginx\n      namespace: ingress-nginx\n   . . .\n</code></pre> <p>The example above shows how custom values are specified for each beach-head service.</p>"},{"location":"usage/deploy-services-clusterdeployment/#templating-custom-values","title":"Templating Custom Values","text":"<p>Using Sveltos templating feature, we can also write templates which can be useful for automatically fetching pre-existing information within the cluster.</p> <p>Example</p> <p> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  name: my-clusterdeployment\n  namespace: kcm-system\nspec:\n  . . .\n  servicesPriority: 100\n  services:\n    - template: motel-0-1-0\n      name: motel\n      namespace: motel\n    - template: myappz-0-3-0\n      name: myappz\n      namespace: myappz\n      values: |\n        controlPlaneEndpointHost: {{ .Cluster.spec.controlPlaneEndpoint.host }}\n        controlPlaneEndpointPort: \"{{ .Cluster.spec.controlPlaneEndpoint.port }}\"\n</code></pre></p> <p>In the example above the host and port information is being fetched from the spec of the CAPI cluster that belongs to this <code>ClusterDeployment</code>.</p>"},{"location":"usage/deploy-services-clusterdeployment/#checking-status","title":"Checking status","text":"<p>The <code>.status.services</code> field of the <code>ClusterDeployment</code> object shows the status for each of the beach-head services.</p> <p>Example</p> <p>Status for beach-head services deployed with <code>ClusterDeployment</code> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  . . .\n  generation: 1\n  name: wali-aws-dev\n  namespace: kcm-system\n  . . .\nspec:\n  . . .\n  services:\n  - name: ingress-nginx\n    namespace: ingress-nginx\n    template: ingress-nginx-4-11-3\n  servicesPriority: 100\n  stopOnConflict: false\nstatus:\n  . . .\n  observedGeneration: 1\n  services:\n  - clusterName: my-managed-cluster\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-12-11T23:03:05Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-12-11T23:03:05Z\"\n      message: Release kyverno/kyverno\n      reason: Managing\n      status: \"True\"\n      type: kyverno.kyverno/SveltosHelmReleaseReady\n    - lastTransitionTime: \"2024-12-11T23:03:05Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre></p> <p>Based on the status above both kyverno and ingress-nginx should be installed in their respective namespaces on the target cluster.</p> <pre><code>\u279c  ~ kubectl get pod -n kyverno\nNAME                                             READY   STATUS    RESTARTS   AGE\nkyverno-admission-controller-96c5d48b4-sg5ts     1/1     Running   0          2m39s\nkyverno-background-controller-65f9fd5859-tm2wm   1/1     Running   0          2m39s\nkyverno-cleanup-controller-848b4c579d-ljrj5      1/1     Running   0          2m39s\nkyverno-reports-controller-6f59fb8cd6-s8jc8      1/1     Running   0          2m39s\n\u279c  ~ \n\u279c  ~ \n\u279c  ~ kubectl get pod -n ingress-nginx \nNAME                                       READY   STATUS    RESTARTS   AGE\ningress-nginx-controller-cbcf8bf58-zhvph   1/1     Running   0          24m\n</code></pre> <p>Note</p> <ul> <li>Refer to Step 7 of Create Cluster Deployment guide for how to access the target cluster.</li> <li>Refer to Service Templates for more detail on what statuses are reported.</li> </ul>"},{"location":"usage/deploy-services-clusterdeployment/#removing-beach-head-services","title":"Removing beach-head services","text":"<p>To remove a beach-head service simply remove its entry from <code>.spec.services</code>. The example below removes <code>kyverno-3-2-6</code> so its status also removed from <code>.status.services</code>.</p> <p>Eample</p> <p>Showing removal of <code>kyverno-3-2-6</code> from <code>ClusterDeployment</code> <pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: ClusterDeployment\nmetadata:\n  . . .\n  generation: 2\n  name: wali-aws-dev\n  namespace: kcm-system\n  . . .\nspec:\n  . . .\n  services:\n  - name: ingress-nginx\n    namespace: ingress-nginx\n    template: ingress-nginx-4-11-3\n  servicesPriority: 100\n  stopOnConflict: false\nstatus:\n  . . .\n  observedGeneration: 2\n  services:\n  - clusterName: wali-aws-dev\n    clusterNamespace: kcm-system\n    conditions:\n    - lastTransitionTime: \"2024-12-11T23:15:45Z\"\n      message: \"\"\n      reason: Provisioned\n      status: \"True\"\n      type: Helm\n    - lastTransitionTime: \"2024-12-11T23:15:45Z\"\n      message: Release ingress-nginx/ingress-nginx\n      reason: Managing\n      status: \"True\"\n      type: ingress-nginx.ingress-nginx/SveltosHelmReleaseReady\n</code></pre></p>"},{"location":"usage/deploy-services-clusterdeployment/#parameter-list","title":"Parameter List","text":"Parameter Example Description <code>.spec.servicesPriority</code> <code>100</code> Sets the priority for the beach-head services defined in this spec (default: <code>100</code>) <code>.spec.stopOnConflict</code> <code>false</code> Stops deployment of beach-head services upon first encounter of a conflict (default: <code>false</code>) <code>.spec.services[].template</code> <code>kyverno-3-2-6</code> Name of the <code>ServiceTemplate</code> object located in the same namespace <code>.spec.services[].name</code> <code>my-kyverno-release</code> Release name for the beach-head service <code>.spec.services[].namespace</code> <code>my-kyverno-namespace</code> Release namespace for the beach-head service (default: <code>.spec.services[].name</code>) <code>.spec.services[].values</code> <code>replicas: 3</code> Helm values to be used with the template while deployed the beach-head services <code>.spec.services[].disable</code> <code>false</code> Disable handling of this beach-head service (default: <code>false</code>)"},{"location":"usage/installation/","title":"Installation Guide","text":"<p>This section describes how to install k0rdent.</p>"},{"location":"usage/installation/#tldr","title":"TL;DR","text":"<pre><code>export KUBECONFIG=&lt;path-to-management-kubeconfig&gt;\n</code></pre> <pre><code>helm install kcm oci://ghcr.io/k0rdent/kcm/charts/kcm --version &lt;kcm-version&gt; -n kcm-system --create-namespace\n</code></pre> <p>This will use the defaults as seen in Extended Management Configuration section below.</p>"},{"location":"usage/installation/#finding-releases","title":"Finding Releases","text":"<p>Releases are tagged in the GitHub repository and can be found here.</p>"},{"location":"usage/installation/#extended-management-configuration","title":"Extended Management Configuration","text":"<p>k0rdent is deployed with the following default configuration, which may vary depending on the release version:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  core:\n    capi: {}\n    kcm: {}\n  providers:\n  - name: k0smotron\n  - name: cluster-api-provider-aws\n  - name: cluster-api-provider-azure\n  - name: cluster-api-provider-openstack\n  - name: cluster-api-provider-vsphere\n  - name: projectsveltos\nrelease: kcm-0-0-7\n</code></pre> To see what is included in a specific release, look at the <code>release.yaml</code> file in the tagged release. For example, here is the v0.0.7 release.yaml.</p> <p>You can optionally customize the default k0rdent configuration, such as specifying a custom OCI registry for the  KCM core components. For more information on how to do this, refer to Extended Management Configuration.</p>"},{"location":"usage/installation/#air-gapped-installation","title":"Air-gapped installation","text":"<p>Follow the Air-gapped Installation Guide to get the instructions on how to perform k0rdent installation in the air-gapped environment.</p>"},{"location":"usage/installation/#cleanup","title":"Cleanup","text":"<ol> <li>Remove the Management object:</li> </ol> <pre><code>  kubectl delete management.kcm kcm\n</code></pre> <p>Warning</p> <p>Make sure you have no k0rdent <code>ClusterDeployment</code> objects left in the cluster prior to deletion.</p> <ol> <li>Remove the <code>kcm</code> Helm release:</li> </ol> <pre><code>  helm uninstall kcm -n kcm-system\n</code></pre> <ol> <li>Remove the <code>kcm-system</code> namespace:</li> </ol> <pre><code>  kubectl delete ns kcm-system\n</code></pre>"},{"location":"usage/management-configuration/","title":"Extended Management Configuration","text":"<p>K0rdent allows you to customize its default configuration by modifying the spec of the <code>Management</code> object. This enables you to manage the list of providers to deploy and adjust the default settings for core components.</p> <p>For detailed examples and use cases, refer to Examples and Use Cases</p>"},{"location":"usage/management-configuration/#configuration-guide","title":"Configuration Guide","text":"<p>There are two options to override the default management configuration of k0rdent:</p> <ol> <li>Update the <code>Management</code> object after the k0rdent installation using <code>kubectl</code>:</li> </ol> <p><code>kubectl --kubeconfig &lt;path-to-management-kubeconfig&gt; edit management</code></p> <ol> <li> <p>Deploy k0rdent skipping the default <code>Management</code> object creation and provide your    own <code>Management</code> configuration:</p> <ul> <li> <p>Create <code>management.yaml</code> file and configure core components and providers.   For example:</p> <p><pre><code>apiVersion: k0rdent.mirantis.com/v1alpha1\nkind: Management\nmetadata:\n  name: kcm\nspec:\n  core:\n    capi: {}\n    kcm:\n      config:\n        controller:\n          defaultRegistryURL: \"oci://ghcr.io/my-oci-registry-name/kcm/charts\"\n  providers:\n  - name: k0smotron\n  - name: cluster-api-provider-aws\n  - name: projectsveltos\nrelease: kcm-0-0-7\n</code></pre>       In the example above, the <code>Management</code> object is configured with custom registry settings for the KCM controller       and a reduced list of providers.</p> </li> <li> <p>Specify <code>--create-management=false</code> controller argument and install k0rdent:   If installing using <code>helm</code> add the following parameter to the <code>helm   install</code> command:</p> </li> </ul> <p><code>--set=\"controller.createManagement=false\"</code></p> <ul> <li> <p>Create <code>kcm</code> <code>Management</code> object after k0rdent installation:</p> <pre><code>kubectl --kubeconfig &lt;path-to-management-kubeconfig&gt; create -f management.yaml\n</code></pre> </li> </ul> </li> </ol> <p>You can customize the default configuration options for core components by updating the <code>.spec.core.&lt;core-component-name&gt;.config</code> section in the <code>Management</code> object. For example, to override the default settings for the KCM component, modify the <code>spec.core.kcm.config</code> section. To view the complete list of configuration options available for kcm, refer to: KCM Configuration Options for k0rdent v0.0.7 (Replace v0.0.7 with the relevant release tag for other k0rdent versions).</p> <p>To customize the list of providers to deploy, update the <code>.spec.providers</code> section. You can add or remove providers and configure custom templates for each provider. Each provider in the list must include the <code>name</code> field and may include the <code>template</code> and <code>config</code> fields:</p> <pre><code>- name: &lt;provider-name&gt; \n  template: &lt;provider-template&gt; # optional. If omitted, the default template from the `Release` object will be used\n  config: {} # optional provider configuration containing provider Helm Chart values in YAML format\n</code></pre>"},{"location":"usage/management-configuration/#examples-and-use-cases","title":"Examples and Use Cases","text":""},{"location":"usage/management-configuration/#configuring-a-custom-oci-registry-for-kcm-components","title":"Configuring a Custom OCI Registry for KCM components","text":"<p>You can override the default registry settings in k0rdent by specifying the <code>defaultRegistryURL</code>, <code>insecureRegistry</code>, and <code>registryCredsSecret</code> parameters under <code>spec.core.kcm.config.controller</code>:</p> <ul> <li><code>defaultRegistryURL</code>: Specifies the registry URL for downloading Helm charts representing templates.  Use the <code>oci://</code> prefix for OCI registries. Default: <code>oci://ghcr.io/k0rdent/kcm/charts</code>.</li> <li><code>insecureRegistry</code>: Allows connecting to an HTTP registry. Default: <code>false</code>.</li> <li><code>registryCredsSecret</code>: Specifies the name of a Kubernetes Secret containing authentication credentials for the  registry (optional). This Secret should exist in the system namespace (default: <code>kcm-system</code>).</li> </ul> <p>Example Configuration:</p> <pre><code>spec:\n  core:\n    kcm:\n      config:\n        controller:\n          defaultRegistryURL: \"oci://ghcr.io/my-private-oci-registry-name/kcm/charts\"\n          insecureRegistry: true\n          registryCredsSecret: my-private-oci-registry-creds\n</code></pre> <p>Example of a Secret with Registry Credentials:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-private-oci-registry-creds\n  namespace: kcm-system\nstringData:\n  username: \"my-user-123\"\n  password: \"my-password-123\"\n</code></pre> <p>The KCM controller will create the default HelmRepository using the provided configuration and fetch KCM components from this repository. For the example above, the following <code>HelmRepository</code> will be created:</p> <pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  labels:\n    k0rdent.mirantis.com/managed: \"true\"\n  name: kcm-templates\n  namespace: kcm-system\nspec:\n  insecure: true\n  interval: 10m0s\n  provider: generic\n  type: oci\n  url: oci://ghcr.io/my-private-oci-registry-name/kcm/charts\n  secretRef:\n    name: my-private-oci-registry-creds\n</code></pre>"},{"location":"usage/management-configuration/#configuring-a-custom-image-for-kcm-controllers","title":"Configuring a Custom Image for KCM controllers","text":"<p>You can override the default image for the KCM controllers by specifying the <code>repository</code>, <code>tag</code> and <code>pullPolicy</code> parameters under <code>spec.core.kcm.config.image</code>: </p> <p>Example Configuration:</p> <pre><code>spec:\n  core:\n    kcm:\n      config:\n        image:\n          repository: ghcr.io/my-custom-repo/kcm/controller\n          tag: v0.0.7\n          pullPolicy: IfNotPresent\n</code></pre>"},{"location":"usage/upgrade/","title":"Upgrading k0rdent to the newer version","text":"<p>Note</p> <p>To upgrade k0rdent the user must have <code>Global Admin</code> role. For the detailed information about k0rdent RBAC, refer to the RBAC documentation.</p> <p>Follow the steps below to update k0rdent to a newer version:</p> <p>Step 1. Create a New <code>Release</code> Object</p> <p>Create a <code>Release</code> object in the management cluster for the desired version. For example, to create a <code>Release</code> for version <code>v0.0.7</code>, run the following command:</p> <pre><code>VERSION=v0.0.7\nkubectl create -f https://github.com/k0rdent/kcm/releases/download/${VERSION}/release.yaml\n</code></pre> <p>Step 2. Update the <code>Management</code> Object with the New <code>Release</code></p> <ul> <li>List available <code>Releases</code>:</li> </ul> <p>To view all available <code>Releases</code>, run:</p> <pre><code>kubectl get releases\n</code></pre> <p>Example output:</p> <pre><code>NAME        AGE\nkcm-0-0-6   71m\nkcm-0-0-7   65m\n</code></pre> <ul> <li>Patch the <code>Management</code> Object with the New <code>Release</code> Name:</li> </ul> <p>Update the <code>spec.release</code> field in the <code>Management</code> object to point to the new release. Replace <code>kcm-0-0-4</code> with the name of your new release:</p> <pre><code>RELEASE_NAME=kcm-0-0-7\nkubectl patch management.kcm kcm --patch \"{\\\"spec\\\":{\\\"release\\\":\\\"${RELEASE_NAME}\\\"}}\" --type=merge\n</code></pre> <p>Step 3. Verify the Upgrade</p> <p>Check the status of the <code>Management</code> object to monitor the readiness of the components:</p> <pre><code>kubectl get management.kcm kcm -o=jsonpath={.status} | jq\n</code></pre>"}]}